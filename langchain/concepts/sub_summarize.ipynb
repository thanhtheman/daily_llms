{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.chains.summarize import load_summarize_chain\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0.8)\n",
    "loader = TextLoader(\"sample.txt\")\n",
    "documents = loader.load()\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)\n",
    "texts = text_splitter.split_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new MapReduceDocumentsChain chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"Langchain is pointless LASR 3 months ago\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"The reason why Langchain is pointless is that it's trying to solve problems on top of technical foundations that just cannot support it.The #1 learning is that there is no reusability with the current generation of LLMs. We're using GPT-4 and 3.5T exclusively.Over the last several months, my team has been building several features using highly sophisticated LLM chains that do all manner of reasoning. The ultimate outputs are very human-like to the point where there is some private excitement that we've built an AGI.Each feature requires very custom handwritten prompts. Each step in the chain requires handwritten prompts. The input data has to be formatted a very specific way to generate good outputs for that feature/chain step. The part around setting up a DAG orchestration to run these chains is like 5% of the work. 95% is really just in the prompt tuning and data serialization formats.None of this stuff is reusable. Langchain is attempting to set up abstractions to reuse everything.\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"of this stuff is reusable. Langchain is attempting to set up abstractions to reuse everything. But what we end up with a mediocre DAG framework where all the instructions/data passing through is just garbage. The longer the chain, the more garbage you find at the output.We briefly made our own internal Langchain. We tore it down now. Again not that our library or Langchain was bad engineering. It's just not feasible on top of the foundation models we have right now.  rchaves 3 months ago\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"100% this! What is worse is that LangChain hides their prompts away, I had to read the source code and mess with private variables of nested classes just to change a single prompt from something like RetrievalQA, and not only that, the default prompt they use is actually bad, they are lucky things work because GPT-3.5 and GPT-4 are damn smart machines, with any other open LLM, things break. I was hoping for good defaults, but they are not, the prompt I wrote over 6 months ago little after the launch of ChatGPT to do some of the same things work much better.Would you have anything you can share with us about those \"several features using highly sophisticated LLM chains that do all manner of reasoning\", I'm really curious about the challenges, the process and insights there  sjnair96 3 months ago\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"Can you share some insights/examples, if you can, on how you improved the prompts? One I feel is particularly poor is the next question generation/past question condensation prompts which are used to refine the user's input based on the history, so that the query includes all the context required for the question, and hence, incorporating \"memory\".  rchaves 3 months ago\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"Yeah I never know where memory goes exactly in langchain, it's not exactly clear all the time. But sure, the main insight I remember is this, take a look at their MULTI_PROMPT_ROUTER_TEMPLATE: https://github.com/hwchase17/langchain/blob/560c4dfc98287da1...It's a lot of instructions for an LLM, they seem to forget an LLM is an auto-completion machine, and which data it is trained on. Using <<>> for sections is not a normal thing, it's not markdown, which probably the thing read way more often on the internet, instead of open json comments, why not type signatures, instead of so many rules, why not give it examples? It is an autocomplete machine!They are relying too much on the LLM being smart because they probably only test stuff in GPT-4 and 3.5, but with GPT4All models this prompt was not working at all, so I had to rewrite it, for simple routing, we don't even need json, carying the `next_inputs` here is weird if you don't need it.So this is my version of it:\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"json, carying the `next_inputs` here is weird if you don't need it.So this is my version of it: https://gist.github.com/rogeriochaves/b67676977eebb1936b9b5c...It's so basic it's dumb, yet it is more powerful, as it does not rely on GPT-4 level intelligence, it's just what I needed  rchaves 3 months ago\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"this inspired me on writing a new section in my project \"Prompts on the outside\" (https://github.com/rogeriochaves/litechain#prompts-on-the-ou...)  Der_Einzige 3 months ago\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"Much of why this stuff is not reusable is that eventually someone in the NLP world is going to properly migrate the features for promopt engineering that the coomers over in stable-diffusion/automatic1111 land have \"pioneered\", such as token weighting, negative prompts, token averaging, or etc. Literally all of these techniques work with regular LLMs (if you don't believe me, see here: https://gist.github.com/Hellisotherpeople/45c619ee22aac6865c...). NLP folks just haven't built the right tooling for it. Particularly sad since there's supposed to be an \"Automatic1111 for LLMs\" project called \"Oogabooga\" but it doesn't have any of the good features.The future of LLM prompting will involve highly specialized and engineered prompts, much as is the case with most images seen on civit.aiWe are all likely to eventually throw away a lot of our current prompts  sandGorgon 3 months ago\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"Automatic111 is the domain of Jupyter - desktop experimentation. When you go into production, there are tons of additional pieces of complexity that start hitting you - like prompt routing. So the problem space is different.We have a simple concept - Generative AI is config management. We model it on top of config management grammar that is proven to work in large production config - jsonnet.A trivial example is this - https://github.com/arakoodev/EdgeChains/blob/main/Examples/r...Do u think this is something that works for you ?  krainboltgreene 3 months ago  \n",
      "              \n",
      "\n",
      "> where there is some private excitement that we've built an AGIThis is a great litmus test for if you need to get a reality check.  teaearlgraycold 3 months ago\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"100% agreed. I've used GPT professionally and we would try out different hosts, AI21, etc. and it there were always clear quality issues with just re-using your prompt and hyperparameters. Some of that was down to other models being lesser quality, but we'd also need to re-tune prompts when upgrading to new OpenAI models for the best effect. It turns out that LLMs aren't quite a commodity.  digitcatphd 3 months ago  \n",
      "              \n",
      "\n",
      "This is precisely why open source models will be limited. Most of the capabilities distinguishing GPT and later Gemini are emergent behaviors from the large parameter count the open source community is saying is not needed (at least for now).  8f2ab37a-ed6c 3 months ago  \n",
      "              \n",
      "\n",
      "Any chance you might have shared some of these hard-earned lessons, so that the rest of us could learn from them as well?  tensor 3 months ago\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"And then they release an updated GPT that breaks all your tuned prompts.  viktor-ferenczi 3 months ago  \n",
      "              \n",
      "\n",
      "That's part of the reason why we need LLMs to run locally (on our own or rented infrastructure). Another reason is protecting the company IP. None of the medium/large corporations want their IP to be leaked to AI providers.  meghan_rain 3 months ago  \n",
      "              \n",
      "\n",
      "Especially where \"updated\" actually means \"lobomized to be less offensive\"  space_fountain 3 months ago  \n",
      "              \n",
      "\n",
      "How do you deal with the prompt iteration phase and how coupled is that to the DAG phase? I've only worked on a few proofs of concept in this phase, but a thing I struggled with was a strong desire to allow non technical colleagues to mess with the prompts. It wasn't clear to me how much the prompts need to evolve in tandem with the the DAG and how much they can exist separately  LASR 3 months ago\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"There are a few increasingly harder things when it comes to prompt customization:1. Prompts ask LLM to generate input for the next step2. Prompts ask LLM to generate instructions for the next step3. Prompts ask LLM to generate the next stepDoing #3 across multiple steps is the promise of Langchain, AutoGPT et al. Pretty much impossible to do with useful quality. Attempting to do #3 very often either ends up completing the chain too early, or just spinning in a loop. Not the kind of thing you can optimize iteratively to good enough quality at production scale. \"Retry\" as a user-facing operation is just stupid IMO. Either it works well, or we don't offer it as a feature.So we stopped doing 3 completely. The features now have a narrow usecase and a fully-defined DAG shape upfront. We feed some context on what all the steps are to every step, so it can understand the overall purpose.#2, we tune these prompts internally within the team. It's very sensitive to specific words. Even things\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"tune these prompts internally within the team. It's very sensitive to specific words. Even things like newlines affects quality too much.#1 - we've found it's doable for non-tech folks. In some of the features, we expose this to the user somewhat as additional context and mix that in with the pre-built instructions.So #2 is where it's both hard to get right and still solvable. Every prompt change has to be tested with a huge number of full-chain invocations on real input data before it can be accepted and stabilized. The evaluation of quality is all human, manual work. We tried some other semi-automated approaches, but just not feasible.All of this is why there is no way Langchain or anything like it is currently useful to built actually valuable user-facing features at production scale.  remmargorp64 3 months ago\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"What if you built a scoring system for re-usable action sequences that are stored in a database, and then have the LLM generate alternate solutions and grade them according to their performance?An action sequence of steps could be graded according to whether it was successful, it’s speed, efficiency, cleverness, cost, etc.You could even introduce human feedback into the process, and pay people for proposing successful and efficient action sequences.All action sequences would be indexed and the AI agent would be able to query the database to find effective action sequences to chain together.The more money you throw at generating, iterating, and evolving various action sequences stored in your database, the smarter and more effective your AI agent becomes.  zicon35 3 months ago\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"Would love to see an open-source version of the internal Langchain you built and what you did differently from an architecture standpoint that made it better in your use-case.  sandGorgon 3 months ago\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"this is precisely the problem i encountered and tried to solve with Edgechains. we think Generative AI is a config management problem (like Terraform or Kubernetes).>None of this stuff is reusable. Langchain is attempting to set up abstractions to reuse everything. But what we end up with a mediocre DAG framework where all the instructions/data passing through is just garbage. The longer the chain, the more garbage you find at the output.chains X prompts X LLMs == pods X services X nodes in Terraform.So we model it on top of config management grammar that is proven to work in large production config - jsonnet.A trivial example is this - https://github.com/arakoodev/EdgeChains/blob/main/Examples/r...Would love to get an example of complex chains (even if u have an ARxiv paper) that you think we could solve in Edgechains-jsonnet ?  applgo443 3 months ago\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"I saw your comment, got curious, and looked at a lot of your old comments. Lots of interesting insights - Thanks for sharing them.If you don't mind me asking, what do you do? I'm a researcher at FAANG working on language models and starting a new company in the space. Would love to connect. Feel free to email me - idyllic.bilges0p@icloud.com  moneywoes 3 months ago  \n",
      "              \n",
      "\n",
      "In that case what pattern do you use for integrations?  minimaxir 3 months ago\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"I have a full-on \"The Problem With LangChain\" blog post in the pipeline, and the reason I made a simple alternative (https://news.ycombinator.com/item?id=36393782) because I spent a month working with LangChain and coming to the conclusion that it's just easier to make my own Python package than it is to hack LangChain to fit my needs.A few bullet points:- LangChain encourages tool lock-in for little developer benefit, as noted in the OP. There is no inherent advantage into using them, and some have suboptimal implementations.- The current implementations of the ReAct workflow and prompt engineering are based on InstructGPT (text-davinci-003), and are extremely out of date compared to what you can do with ChatGPT/GPT-4.- Debugging a LangChain error is near impossible, even with verbose=True.- If you need anything outside the workflows in the documentation, it's extremely difficult to hack, even with Custom Agents.- The documentation is missing a lot of relevant detail (e.g. the\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"to hack, even with Custom Agents.- The documentation is missing a lot of relevant detail (e.g. the difference between Agent types) that you have to go diving into the codebase for.- The extreme popularity of LangChain is warping the entire AI ecosystem around the workflows to the point of harming it. Recent releases by Hugging Face and OpenAI recontextualize themselves around LangChain's \"it's just magic AI\" to the point of hurting development and code clarity.Part of the reason I'm hesitant to release said blog post is because I don't want to be that asshole who criticizes open source software that's operating in good faith.  TeMPOraL 3 months ago\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"> Part of the reason I'm hesitant to release said blog post is because I don't want to be that asshole who criticizes open source software that's operating in good faith.Beyond the \"extreme popularity of LangChain is warping the entire AI ecosystem around the workflows to the point of harming it\", hasn't it recently become an attractor for substantial amount of investment money? I'm not saying you should be an ass about it, but the ecosystem will keep getting warped further if knowledgeable people won't speak up, and LangChain doesn't seem to be a random small open source project anymore.EDIT:From https://news.ycombinator.com/item?id=36647616:> Remember, this is the project that raised ~$30m from Benchmark and Sequoia.I think we're past the point of good-faith, small, volunteer-made open source project. If it can take $30M, it can stand some informed criticism too.  minimaxir 3 months ago\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"I'm not worried about LangChain not taking criticism well, it's more the fanboys who have a vested interest in maintaining the status quo and I don't have the free time to deal with annoying \"you're just nitpicking because you're jealous\" and \"it's open source, why don't you just make a PR to fix everything instead of whining?\" messages.  ntindle 3 months ago  \n",
      "              \n",
      "\n",
      "This is our worry with building Auto-GPT as well. We have had a number of rather involved discussions on why we don’t use it. I’d love if you published so I can point to it rather than rehashing it every few days.  layer8 3 months ago  \n",
      "              \n",
      "\n",
      "You can just ignore those fanboy messages?  tryfinally 3 months ago\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"You can just ignore those fanboy messages?  tryfinally 3 months ago  \n",
      "              \n",
      "\n",
      "> Debugging a LangChain error is near impossible, even with verbose=True.(A while ago) I tried using LangChain and shortly gave up after not finding any way whatsoever to actually debug what’s going on under the hood (eg. see the actual prompts, LLM queries). It’s pretty ridiculous that this isn’t basic functionality, or at least it isn’t very discoverable.I cannot imagine spending extended time with a framework without knowing what the internals are doing. I do realize this isn’t achievable on all levels with LLMs, but introducing more black boxes on top of existing ones isn’t solving any problems.  cetico 3 months ago  \n",
      "              \n",
      "\n",
      "Can’t you simply create a custom LLM interface that does whatever query logging you want?  ntindle 3 months ago\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"We’ve had a lot of similar concerns when working on Auto-GPT and have been repeatedly asked why we don’t use it. You’ve solidified a lot of the reasons it’s not fit for purpose for large complex projects.We’ve received a lot of commentary on our unwillingness to use it, and I don’t blame you for being hesitant. I don’t want to be the open-source project that says it’s not good when it’s not suitable for our uses.  ajcwebdev 3 months ago\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"Arr matey, ye might be taken aback, but this here post is singing the praises of LangChain, loud and clear. You only spot such dreadful slander taking wings when a project be making mincemeat of its rivals, and someone's tender feelings be getting a bruising.For that poor soul, it's like a slap in the face from the mighty Poseidon himself. The thriving project dares to steer its course in a way that ruffles their precious sensibilities. The audacity! Since they be the compass of all that's right, the project must be heading for the rocks, not them. Why would any sane sailor hitch their fortunes to this monstrous beast, when it's not charted on their blessed map?But in a world soaked to the bone with crowd follies and tribal loyalties, the voice of the multitude sometimes manages to ring out as one, and for good reason, mark me words. Cast your eyes on the likes of React, Kubernetes, and Tailwind.These examples, like our beloved whipping boy LangChain, skilfully merge a motley of\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"and Tailwind.These examples, like our beloved whipping boy LangChain, skilfully merge a motley of tactics from the teeming ecosystem, distilling them into a chart that's simpler and more intuitive, though a tad odd and confining.As it sails the high seas, growing and evolving, brace yerself for the titanic task of keeping the code shipshape. But our chummy critic, bless their heart, can't spot the shining treasure that's clear as day to the rest of us simple seafarers. They'd sooner believe it's a devilish trick or that the developers finding riches in it are either lost at sea or plain daft.This stirs a merry storm of cognitive dissonance in their noggin. It becomes their holy mission to persuade themselves that it's the rest of the crew caught in a dreadful mirage, and they're the sole beacon of sanity in a mad world!They nimbly dodge Occam's Cutlass, baffled by how the whole crew could go stark raving mad in harmony. And then comes the climax: a breathtaking revelation where\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"could go stark raving mad in harmony. And then comes the climax: a breathtaking revelation where they're forced to grapple with the unsettling truth of their own tunnel vision and stubborn notions. What a sight to behold, arr!> That's my thoughts anyway. But filtered through a slightly Irish Pirate to make it sound a bit less like I think you're take is bad and you should feel bad about it. It's great you're helping get the word out about LangChain to more people though.  PUSH_AX 3 months ago\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"> Part of the reason I'm hesitant to release said blog post is because I don't want to be that asshole who criticizes open source software that's operating in good faith.I agree with your restraint, this feels like it might be more productive in another format. Ultimately this either needs to be broached with the maintainers or an alternative should be started.  shepardrtc 3 months ago  \n",
      "              \n",
      "\n",
      "The maintainers have 30 million reasons to ignore detractors.  pkc_official 3 months ago  \n",
      "              \n",
      "\n",
      "What would you be looking for in an alternative?\n",
      "More control?  shepardrtc 3 months ago  \n",
      "              \n",
      "\n",
      "> Part of the reason I'm hesitant to release said blog post is because I don't want to be that asshole who criticizes open source software that's operating in good faith.Please release it.  People need to see these things BEFORE they get sucked into building an entire product around it.  vegetax 3 months ago\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"yes! whenever I try to understand langchain,  it makes me more motivated to create it myself.  jangletown 3 months ago\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"I agree, I really don’t like LangChain abstractions, the chains they say are “composable” are not really, you spend more time trying to figure out langchain than actually building things with it, and it seems it’s not just me after talking to many peopleTheir code seems all rushed, and seems it worked out for initial popularity, but with their current abstractions I personally don’t think it’s a good long term framework to learn and adoptThat’s why I built my own alternative to it, I call it LiteChain, where the chains are actual composable monads, the code is async streamed by default not ducktaped, it’s very bare bones yet but I’m really putting effort on building a solid foundation first, and having a final simple abstractions for users that don’t get in the way, check it out:https://github.com/rogeriochaves/litechain  ibains 3 months ago\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"Why is this just not ETL, why do you need anything here? There is no new category or product needed here.  jangletown 3 months ago  \n",
      "              \n",
      "\n",
      "Just saw the video you shared on the other comment using prophecy, very coolGenerally I don’t care much about the embedding and retrieval and connectors etc for playing with the LLMs, I imagined much more robust tools were available already indeed, my focus was more on the prompt development actually, connecting many prompts together for a better chain of thought kinda of thing, working out the memory and stateful parts of it and so on, and I think there might be a case for an “LLM framework” for that, and also a case for a small lib to solve it instead of an ETL cannonHowever, I am indeed not experienced with ETLs, have to play more with the available tools to see if and how can I do the things I was building using them  rchaves 3 months ago\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"hmmm, just had a chat with GPT-4, it didn't seem convinced that ETLs would do well the same things that LiteChain is trying to achieve: https://chat.openai.com/share/88961bd1-8250-45f0-b814-0680ba...I'd be happy to see some more examples of LLM application building on ETLs like the video you shared though  krawczstef 3 months ago  \n",
      "              \n",
      "\n",
      "Totally! As a person driving a project like https://github.com/DAGWorks-Inc/hamilton I couldn't agree more!  ibains 3 months ago\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"It is pointless - LlamaIndex and LangChain are re-inventing ETL - why use them when you have robust technology already?1. You ETL your documents into a vector database - you run this pipeline everyday to keep it up to date. You can run scalable, robust pipelines on Spark for this.2. You have a streaming inference pipeline that has components that make API calls (agents) and between them transform data. This is Spark streaming.Prophecy is working with large enterprises to implement generative AI use cases, but they don’t talk so much on HN.Here’s our talk from Data+AI Summit: Build a Generative AI App on Enterprise Data in 13 Minutes https://www.youtube.com/watch?v=1exLfT-b-GMHere’s a blog/demo \n",
      "https://www.prophecy.io/blog/prophecy-generative-ai-platform...  lmeyerov 3 months ago\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"We also do platform & customer work there (cool pipelines to feed  louie.ai or real-time headless versions), and agreed those pipelines have simple uses of LLM where langchain is mostly useful just for a vendor neutrality. Think BYO LLM as it is now a zoo. Basically apache nifi or spark streaming with simple LLM & vector DB call outs. Our harder work here is more at the data engineering level.But....a lot of our louie.ai work happens for less trivial scenarios where it isn't just the ETL NLP 2.0 tier . That logic is much more complicated, so structured programming abstractions matter a LOT more for AI-style business logic. Think talk to your data and generate on-the-fly analytics pushdown with an interactive data viz UI. That's.. a lot of code.  techwizrd 3 months ago\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"I agree that it's a little silly, but I mostly use it to abstract over BYO LLMs and extract information from documents. It's nice to be able to quickly prototype something and swap out the underlying language model than set up a whole pipeline with Apache Tika, ETL, etc. Once the idea is feasible, then sure.That said, langchain is really inefficient and I often find I can re-implement the pieces I need much faster than dealing with langchain's bugs and performance issues.  ibains 3 months ago  \n",
      "              \n",
      "\n",
      "That’s assuming you’re not using low-code. There are inbuilt connectors to read data, transform data, read/write to pinecone, make api calls to LLMs. It is much faster to prototype with Prophecy.io  codeptualize 3 months ago\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"I have looked for the value and never really found it.It seems to mostly be (bad) abstractions around things you could easily do without langchain.Take one of the most important llm things: prompt templates. What does langchain add over a simple function and an f string? Maybe I'm missing the point, but I can't find anything.Anyway, it seems people like it so who am I to judge, but I don't like making our codebase  dependent on a huge new library with unnecessary abstractions and little or no value add.  TeMPOraL 3 months ago\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"> Take one of the most important llm things: prompt templates. What does langchain add over a simple function and an f string? Maybe I'm missing the point, but I can't find anything.Seconding.Ever since learning about it, then seeing a co-worker use it for some simple embedding job (and being impressed in how few lines of code it took, but that's actually not thanks to LangChain), then reading its docs end-to-end, about once a week, I find myself going through the following sequence of thoughts:1. Alright, let's set up LangChain and implement my ${most recent harebrained idea};2. Oh, but it's in Python. I don't like Python, I don't know Python, I hate dealing with its dependency issues even more than with NPM ones. Could I do things I need from it directly in ${my preferred environment, which half the time is just Emacs}?3. Wait a minute. Chaining \"DAGs\" the way it does is basically equivalent to a sequence of function calls in a while loop, occasionally mixed with some if/else or\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"equivalent to a sequence of function calls in a while loop, occasionally mixed with some if/else or goto. Generating prompts is... string interpolation that can be wrapped in a helper function. LMAO.4. No, really. Why bother? The only useful thing here seems to be discoverability - i.e. a list of toolkits it supports, and said support working as intro 101 tutorial. Given the surface areas of those plugins are so small, I can literally wrap what I need in a bunch of functions, and then do the \"chain\" part as... plain old sequential code.So yeah, right now, I think about the only value this project has is in being a convenient list of AI tools with examples of using their APIs. Everything else seems better done either by coding it directly, or (for certain needs) by building up a more complex dataflow framework.  codeptualize 3 months ago\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"Fair assessment. Maybe you have found the value: it’s a library of code snippets. There is truly something to say for that, but maybe it had worked better as a documentation site (kinda ironic as their docs are not super great).> plain old sequential codeThis!! Besides the templates this is the most confusing thing, it is indeed the same as running lines of code sequentially.P.S. for python dependencies, check out Poetry, not perfect but a lot more sane than the other options.  davesque 3 months ago  \n",
      "              \n",
      "\n",
      "Honestly, it's hard to even tell if people really like it or if the LangChain team have just done a really good job of evangelizing for it. I saw that they did some kind of interview with Andrew Ng the other day. I feel like that sort of thing doesn't just happen by accident, but because someone actively reached out to him, especially considering that LangChain has only been on the scene for a couple of months.  codeptualize 3 months ago\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"True, you gotta respect the marketing, they definitely got that covered and actually created some real value there, a $10M seed round to be precise.I think they got the timing right and the idea is kinda alluring as well: have everything standardized, pluggable and swappable sounds neat. Just doesn't really work.They are also doing a great job at maintaining the mirage of adding value. Templates for example will work totally fine, as they are just f strings. It seems there are plenty of people who don't really think twice about it, and they have captured that audience very well.There is also a low barrier to contribute as everything is kinda basic, that must help a lot as well.  minimaxir 3 months ago  \n",
      "              \n",
      "\n",
      "LangChain has been active since last year (after the ReAct paper was released), but gained most of its popularity after the ChatGPT hype.  trolan 3 months ago\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"I actually found a lot of value when I left the LangChain ecosystem and started using jinja templates. The syntax from home assistant moved over and I can just pass dictionaries to render a prompt now.  rdli 3 months ago  \n",
      "              \n",
      "\n",
      "I found it helpful for prototyping and learning some basics, but I quickly found the abstractions were not useful and had to implement my own.  saulpw 3 months ago  \n",
      "              \n",
      "\n",
      "I agree, and that's why I've been working on AIPL[0].  Our first v0.1 release should be in the next few days.  https://github.com/saulpw/aiplIt's basically just a simple scripting language with array semantics and inline prompt construction, and you can drop into Python any time you like.  codeptualize 3 months ago\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"With peace and love, why would I not just write some code?LLM pipelines are not very complex: it's string manipulation, api calls, and storage, there is not much more too it. All are quite easy to do, often needing nothing else but the standard library.For more complex cases or bigger scale you have a plethora of battle tested solutions to manage things like queues, back off and retries, concurrency, etc.Maybe it's me missing the point (again), but I wonder what the added value is of learning a new language to do things that are super easy to do in Python/JS/whatever my language of choice is? Or maybe I'm just not be your target audience, very possible.  Reticularas 3 months ago\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"I get this feeling too \"maybe I'm not the target audience\"\n",
      "This feeling is followed closely by \"Who is the target audience?\". Some abstract concept of an audience doing complex LLM work to accomplish... something?The value in statistics analysis for LLMs is clear, the value in chaining responses is very unclear.  dereg 3 months ago\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"Since last year, before I heard about langchain, I've been building my own stack of tooling for my own LLM projects that probably now covers about 10-20% of Langchain's functionality. I heard about Langchain earlier this year and groaned, thinking that I did a lot of work for nothing....Then I actually used langchain. I was shocked at how poorly performant the code is. Some operations took 10x longer than how I did it, and all the while producing worse results. As tempting as it is to just roll with langchain from day one, I'd highly advise against it. Think deeply about what you're actually trying to accomplish and instead of just injecting langchain in the middle of everything as this messy, amorphous glue code thing.  Tostino 3 months ago\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"Yeah I've got a few thousand lines of langchain code now for a data cleaning pipeline... I've been fighting it every step of the way. Trying to replace sections of the pipeline to use a local LLM instead of OpenAI has had me have to replace the templates entirely, the chat based templates won't allow me to assign the proper user/assistant names, so the performance for the local LLM is terrible (stupid). They have zero actual composibility when you look at it slightly differently than they expect.It's a useless abstraction for every single purpose I've actually tried it for.Will be extricating it from my code base as soon as I find something else that works any better.  sandGorgon 3 months ago\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"I have an attempt in the same domain, would love feedbackWe think - Generative AI is config management. We model it on top of config management grammar that is proven to work at K8s/Terraform scale - jsonnet.https://github.com/arakoodev/EdgeChains/blob/main/Examples/r...Prompts live outside the code. We didnt invent a new markup - we used jsonnet which is used in large scale kubernetes and has a grammar that has been well tested for config mgmt.  pkc_official 3 months ago  \n",
      "              \n",
      "\n",
      "We thought the same thing with https://neum.ai - config management/infra as code for your LLM app, using plain JSON, though we are abstracting even more what LangChain does. Not suitable for the audience here given the level of customizations some folks want.  Tostino 3 months ago  \n",
      "              \n",
      "\n",
      "Oh man, something written in a language I actually know and use?! Will check it out.  FemmeAndroid 3 months ago\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"I had this exact same experience.  I was happy to move to something good, but I couldn’t find a lot of benefit.  Maybe I’m missing something, but the added complexity is not worth it to me for what it provides.  nbbaier 3 months ago  \n",
      "              \n",
      "\n",
      "Do you have your stack described somewhere?  senko 3 months ago  \n",
      "              \n",
      "\n",
      "Remember, this is the project that raised ~$30m from Benchmark and Sequoia.There was a controversial \"quality doesn't matter for software products\" post and discussion[0] here on HN a few days ago and this is a beautiful example.Product may matter eventually, but you can sure surf the hype for a long time before the reckoning comes (and if you're lucky, you may even be able to get someone else to hold the bag then).0: https://news.ycombinator.com/item?id=36615286  weyj4 3 months ago\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"Thanks for this post, I hadn't seen the linked conversation when it happened. Here's my reading. The conversation blog post you linked begrudgingly points out that companies aren't sunk by bugs, technical debt, or inefficient development practices. If you have a good sales team and a product that people want, need, or will be forced to use, you can succeed even if you have to burn money on dev, ops, customer support.\n",
      "However I think what this langchain conversation is about is overeager VCs and a product that maybe doesn't do anything we need. It's not that langchain is slow or hard to use; it's that we can just do this stuff with Python or whatever. I don't have enough langchain experience to substantiate these claims but I think that's what I'm reading here.  RC_ITR 3 months ago\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"We’ve seen the top dog fall from grace in the VC world before (KPCB missing social and mobile after a truly epic run in the 90s)Is that what we are starting to see for Sequoia?Like KP, I doubt they’ll fold, but it would be interesting to live in a world where sequoia is just top half instead of top 1.  jchonphoenix 3 months ago  \n",
      "              \n",
      "\n",
      "Sequoia never figured out their edge in the enterprise world. They likely won't fall from grace since they're still investing in the space, but their filter is certainly poorer than their other areas of investment.  nojvek 3 months ago  \n",
      "              \n",
      "\n",
      "KPCB = Kleiner Perkins Caufield & Byers (KPCB)Today I learnt.  nextworddev 3 months ago  \n",
      "              \n",
      "\n",
      "Any ideas what valuation range  that might imply?  PhoenixReborn 3 months ago  \n",
      "              \n",
      "\n",
      "The rumored valuation was around $200M, and the product is pre-revenue right now. So that seems pretty ridiculous.  ShamelessC 3 months ago\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"I’m curious - who gets screwed over the most here? Is it investors who got tricked into over valuing? Or langchain who now can’t meet their expected revenue targets and will be forced to pivot?Speaking in hypothetical terms of course. I’m assuming the langchain folks are probably paying themselves pretty well and not working super hard (at least not on engineering stuff)?  PhoenixReborn 3 months ago  \n",
      "              \n",
      "\n",
      "Usually the investors - the funding amount means that the founders and early employees can get paid a living wage at least (as a former VC-backed founder I can say that they probably don't get paid exorbitantly). But investors will get rinsed if the company can't either reach IPO or raise a subsequent round at a higher valuation. This becomes very difficult when the company is already worth $200M on paper, and need to get the revenue to a point where that valuation is justified.  nextworddev 3 months ago\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"What is the product?  viktor-ferenczi 3 months ago  \n",
      "              \n",
      "\n",
      "Exactly  krychu 3 months ago\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"Using an LLM framework at this moment doesn’t make sense and can be damaging, in my humble opinion. Ways to extract value from LLMs are in early exploration stage. Look at research in prompting: chain of thought, react, reflection, tree of thoughts, zero vs few hot etc. Then completion vs conversational interfacing. Then memory management via vector databases and prompt expansion vs compression vs progressive summarization etc. All these are fairly recent developments. They are not abstractions worth cementing,  this is search and creative phase. LLMs threw everything in the air, but the dust is far from settling. I think it’s important to recognize the phase we’re in and pick your weapon accordingly. You have to stay nimble and light, ready to experiment with a new idea that will come out next week. You should be hacking these things together by yourself. If you pick a framework at this stage know that the framework will have to pay the price of trying to cement things in the times\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"stage know that the framework will have to pay the price of trying to cement things in the times of storm. And you’ll be a few steps behind. Of course this is my personal take.  p1nkpineapple 3 months ago\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"> Look at research in prompting: chain of thought, react, reflection, tree of thoughts, zero vs few hot etc. Then completion vs conversational interfacing. Then memory management via vector databases and prompt expansion vs compression vs progressive summarization etc.Wow, that's a list of things that's completely new to me - thanks! Do you have any particular resources that you'd recommend for learning about these different topics?  krychu 3 months ago\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"Chain of thought: https://arxiv.org/abs/2201.11903\n",
      "ReAct: https://arxiv.org/abs/2210.03629\n",
      "Reflexion: https://arxiv.org/abs/2303.11366\n",
      "Tree of thoughts: https://arxiv.org/abs/2305.10601Good video on \"Tree of thoughts\" which also reviews / puts it in the context of other methods: https://www.youtube.com/watch?v=ut5kp56wW_4Completion vs conversational interface is something you can read about in the OpenAI API documentation.For the remaining things I don't have single specific pointer at hand.  nbbaier 3 months ago  \n",
      "              \n",
      "\n",
      "Would also love to see resources!  arpowers 3 months ago  \n",
      "              \n",
      "\n",
      "Proper term is that it's a \"false abstraction\"...It abstracts some work only to introduce introduce it's own API (which is ultimately more complicated, less documented, introduces limits and constraints, and comes with its own bugs and dev politics)...But nobody asked me, keep using it  joshka 3 months ago\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"To coin a portmanteau: that's the Fauxcade pattern ;)(though it does exist already)\n",
      "https://www.urbandictionary.com/define.php?term=fauxcade  teaearlgraycold 3 months ago\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"I have done a bit of research trying to figure out why anyone would use langchain. The main two reasons I’ve found are these:1. Newbies that want to play with LLMs don’t know where to start or what the major building blocks even are. Despite the complaints here about documentation their getting started docs will walk you through the concepts. Going from total ignorance and confusion to now having a rough understanding of loading a prompt with chat history, using an embeddings database, calling a completions endpoint, etc. will make people feel accomplished. And then lang chain has earned some loyalty just because they were there for you first.2. In the case that you don’t know which embedding db, AI host, or model you want to use you can quickly swap those in and out and measure the results. That means there’s little reason to complicate your back end code with lang chain (I’ve always just written my own abstraction layer to make this possible with very few lines of code). But for a\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"just written my own abstraction layer to make this possible with very few lines of code). But for a python notebook it can make sense.  robbywashere_ 3 months ago\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"Sad to hear the news, I have 7 years of langchain experience on my resume, now what am I going to do!?  beepbooptheory 3 months ago  \n",
      "              \n",
      "\n",
      "You should probably not mention it by name on a resume anyway, but emphasize your actual skills, experience, and familiarity with the lower level technologies and APIs that underly it.This hype train isn't going to last forever, and it's probably better to advertise sustainable, evergreen skills rather than being an expert in the flavor of the week. With that much experience, you shouldn't have to feel tied to any one company's software.  ta988 3 months ago  \n",
      "              \n",
      "\n",
      "It was a joke, langchain was released last year.  beepbooptheory 3 months ago  \n",
      "              \n",
      "\n",
      "Ha! I guess I don't really get the joke, but relieved for gp's sake it is one, despite the egg on my face.  pessimizer 3 months ago  \n",
      "              \n",
      "\n",
      "*didn't really get the joke,If you don't get it now, I'm worried about you:)  beepbooptheory 3 months ago\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"No no, that is the right tense! I thought initially it was like light sarcasm targeting the conceit of the reddit post, i.e., \"how could it be pointless if I have amassed all this experience.\" Beyond that.. not sure (I'm not that smart).Perhaps its just something like: \"the scandal that this library is bullshit amounts to not a lot considering it's still pretty new thing.\"  But the, erm, strong showing of downvotes of my original post makes me think its a better bit than that!Indulge me with the joke if you'd like, but don't worry about me! I'm doing pretty good, despite my slow mind.  drdrey 3 months ago  \n",
      "              \n",
      "\n",
      "The context for the joke is that some job listings have inflated requirements, like N years of experience from framework/language X that realistically nobody has (or in this case, can have)  skwirl 3 months ago  \n",
      "              \n",
      "\n",
      "Whooosh  ravenstine 3 months ago\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"Whooosh  ravenstine 3 months ago  \n",
      "              \n",
      "\n",
      "I had the same thought about Langchain, and it's essentially my same criticism of most wrapper libraries and SDKs. What are they actually doing that can't be just as easily done with straight up string manipulation and HTTP requests? Usually very little. ORMs might be one of the few exceptions in some cases.Even so, ORMs sell a similar false promise to Langchain, which is that you can \"easily\" swap out the underlying thing; a migration that is rare in practice and almost always not that simple.You might not need Langchain.  refulgentis 3 months ago\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"It's a really interesting question: the converse of this is its _really_ tricky nailing all this down, much harder than you'd think -- see the GPT4 GA thread earlier this week for people who swear up and down the OpenAI API acts in bizarre ways, you'll note that it almost seems fantastical that it could be that bizarre: it is fantastical. If you're truly used to it, it reads like \"hey I have auto retries enabled in my HTTP client framework. p.s. whats 'context size'?\"But Langchain is far, far, _far_ away from being truly helpful with that. It's glorious that we're 5 months into GPT-4 and most people either got bored or are building on rickety rushed structures in Python to rush out a proof of concept web app.  resiros 3 months ago\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"I believe the abstractions in Langchain are inherently flawed. The core problem resides in the composability of chains. While it offers a handy way to create prototypes, it becomes restricting when you desire to modify a specific element within the chain. The hierarchical design of chains in Langchain conceals the component you wish to alter and obscures the parts developers might want to adjust, making the process of experimenting and refining the pipeline difficult.The optimal abstraction for LLM apps, in my view, should resemble a DAG or a state machine. This alternative exposes the distinct stages in the pipeline rather than masking them in a hierarchy. Yes, adopting this new abstraction might lead to more code but it offers superior control. It's hardly surprising that many users start prototyping with Langchain, but then, when ready, they clone the prompts and construct their own systems.Fixing this fundamental issue would be very difficult. It would necessitate reworking the\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"systems.Fixing this fundamental issue would be very difficult. It would necessitate reworking the library from the ground up.  joshka 3 months ago\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"Take a look at https://promptfile.org/ for an alternative approach to the prototyping -> app flow.  sandGorgon 3 months ago  \n",
      "              \n",
      "\n",
      "Promptfile is written in markdown, which is unsuited for templates and config management.I have an attempt in the same domain, would love feedbackWe didnt invent a new markup - we used jsonnet which is used in large scale kubernetes and has a grammar that has been well tested for config mgmt.https://github.com/arakoodev/EdgeChains/blob/main/Examples/r...Prompts live outside the code.  pharmakom 3 months ago  \n",
      "              \n",
      "\n",
      "A DAG is not expressive enough for some applications. Lang chain is actually a poorly implemented free monad.  version_five 3 months ago\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"I don't know enough to agree that it's pointless, but I'd agree that when I looked at it I saw a lot of abstraction of already simple stuff (like the examples the post gives) and decided that for what I was doing it would be faster and easier to understand to just write my own python script. Though I can picture for very inexperienced developers the abstractions may be helpful short term?  paulgb 3 months ago  \n",
      "              \n",
      "\n",
      "I had a similar experience; looked into using it for something, and felt that it would be easier to recreate the things I would use langchain for in Jinja2, than it would be to reshape my code to the interface that langchain wanted.  hwchase17 3 months ago\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"LangChain co-founder here. There's lots of good feedback here (that also resonates with previous feedback) that we're working hard to address. On some key points:- We genuinely appreciate all the thoughtful criticism and feedback. Our goal is to make it as easy as possible to build LLM applications (both prototypes and production-ready applications), and if we're falling short in an area we'd much prefer to hear it rather than not. We don't have the bandwidth to respond to all feedback directly, but we do (1) appreciate it, and (2) try to address it as quickly as possible.- Documentation: we've heard this for a while now, and have been working to improve it. In the past ~3 weeks we've revamped our doc structure, changed the reference guide style, and worked on improving docstrings to some our more popular chains. However, there is a still a lot of ground to cover, and we'll keep on pushing. Feedback on which specific chains/components need better documentation is particularly helpful-\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"Feedback on which specific chains/components need better documentation is particularly helpful- Customizability: we need to make it easy to customize prompts, chains, and agents. We're thinking of changes to more easily enable this - better documentation, more modular components. We'll up the priority of this.- Other tooling: there are general difficulties in building LLM applications that aren't strictly related to langchain, such as debugging and testing. We're working on building separate tooling to assist with this that we hope to launch soon.  morgango 3 months ago\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"I think Langchain is like democracy, everyone complains about it and tries to poke holes in it but it is clearly better than all the alternatives.Once I got \"into\" langchain and how it did things my life as a developer got infinitely easier.  It is true that it is doing a lot of things that you \"could\" do elsewhere, but that is kind of the point of a library.  For example, it makes it incredibly easy to switch between vector datastores or embeddings, with just a tiny code change.  I love that.Look at how much code it takes to actually get something done.  It makes it trival to take a file (or a number of files), chunk them, and load them into a vector store.  Sure, I could write and maintain the code to do that, but why?While I did find it challenging to get started with Langchain, it was more a lack of understanding of the ecosystem than anything else.  Great abstractions aren't going to shield me from that without restricting choice.  The documentation has improved noticeably in the\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"shield me from that without restricting choice.  The documentation has improved noticeably in the last few weeks.Great work, it is very much appreciated by the non-HN crowd.  Don't let this feedback get you down.  lmeyerov 3 months ago\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"I've definitely got heartache here, and they merit criticism, but it is realWe need a lot of pluggability to support diff vendor LLMs and BYO LLMs in Louie.ai, so having langchain has been nice for helping code to interfaces vs vendor lockin. It definitely has growing pains - ex: sync & multithreading is important for us so we are generally coding around langchain while that smooths out. Likewise, we ended up building much of our conversational and multitool capabilities as custom libraries vs using theirs for similar quality reasons. We can't use any of the codegen capabilities because they are massive security holes so doing our own work there too.If anyone is into that kind of work (backend, AI & web infra, ...), definitely hiring for core platform & cool customer projects here: louie.ai / Graphistry.com/careers  lmeyerov 3 months ago\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"Also, the thread title is unintentionally funny: I'd like the interface to be more functional so we can write truly 'point-free' pipelines, especially around areas like memory. Ex: When dealing with multithreading, that makes it a lot safer. There are projects exploring that, but langchain is winning as a pluggable interface for many new LLM providers.Edit: link - https://en.wikipedia.org/wiki/Tacit_programming  crosen99 3 months ago  \n",
      "              \n",
      "\n",
      "I keep going back to LangChain thinking it just hasn't found its legs yet, but every time I do I retreat exasperated.  I don't find their abstractions useful or intuitive, and their documentation is woefully scattered and incomplete.  Things are moving so quickly with LLMs that theirs is no easy task, but so far they haven't really cracked the nut of making LLM app development easier.  CGamesPlay 3 months ago\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"Langchain was useful to me personally for two reasons: using their prompt templates as a starting point for my own, and seeing how their “tools” were built to learn about good Python libraries to build my own tools with.Viewed through this lens, LangChain was more a “sample codebase” than a library for me, and it was reasonably good for that.  Tostino 3 months ago  \n",
      "              \n",
      "\n",
      "When you actually get down to it, they're pointless and counterproductive. Templates are pretty useless and have inconsistently implemented features which make them non-compatible with different LLM backends without changing all your code. Honestly, cannot recommend avoiding the library all together high enough.  rgrieselhuber 3 months ago  \n",
      "              \n",
      "\n",
      "I've built a few LLM-based projects now and quickly discovered that Langchain was overkill (and not even very good overkill) for my use cases. I thought it was just me, glad to hear it's not.  codeptualize 3 months ago\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"Coming from a frontend background this reminds me a lot of the frontend situation some years ago when it was super common to npm install the stupidest pointless packages.Of course some people still do, but hard lessons were learned and all experienced people I know are a lot more mindful and cautious what dependencies they add.It seems to me that this space, and maybe data science more broadly, is currently in that situation. Maybe it's the lack of coding skills, maybe it's the transition from one-off research notebooks to production applications, or maybe it's just the norm to have big do it all libraries (like pandas, scikit etc), idk, but I expect the same lessons will be learned.For Langchain I wonder how they will keep up with changes in all the things they have their abstractions on. I guess having a huge community helps, but that doesn't help you with compatibility. It would not surprise me if that will get really ugly.  ShamelessC 3 months ago\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"The problem is that coders are used to dealing with code. GPT-4 is a robust processor for semantics as conveyed by strings of characters. The whole point is that you don’t need code. You just ask it what you want.But programmers love to think they can still make improvements to such a system using code. In reality, any improvements that can be made are the responsibility of f-strings and/or a templating mechanism and even that may be overkill in many cases.  codeptualize 3 months ago  \n",
      "              \n",
      "\n",
      "Often there is still a lot of programming to do, as a lot of the use cases involve some sort of integration into a bigger systems, or you might have to process larger quantities of data which gets you into queues and for example managing rate limits.That said I 100% agree that often the basic tools are sufficient to solve these problems, and where it gets complex there are many battle tested solutions.  brando90 3 months ago\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"Claude 2.0 summary of this:Here are a few key points from the Hacker News discussion on Langchain:- Many commenters feel Langchain introduces unnecessary abstraction and indirection, making simple LLM tasks more complex than just using Python and APIs directly. The abstractions don't seem to provide much real benefit.- There are critiques about Langchain's poor documentation, lack of customizability, and difficulty debugging. The rapid pace of updates is also seen as problematic.- However, some point out benefits like easy swapping of components (models, vector stores, etc) and potential future improvements. A few see value in it for learning or quick prototypes.- Overall sentiment seems quite negative on the usefulness of Langchain for real production systems, with many sharing they tried it but went back to coding LLM workflows directly. Some think it's hyped and reflects poor technical choices.- But a counterpoint is that its popularity makes it a safe choice to build on initially,\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"choices.- But a counterpoint is that its popularity makes it a safe choice to build on initially, despite architectural issues. The crowd may offset other downsides.In summary, the overall vibe of the discussion appears quite critical of Langchain's technical merit and production readiness, though there is some defense of its prototyping and educational potential.  sandkoan 3 months ago\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"Wholeheartedly agree—it adds layers of bloat and abstraction between you and the actual ReAct pattern, which can be trivially implemented in maybe 50 sloc.  m3kw9 3 months ago  \n",
      "              \n",
      "\n",
      "Langchain is as pointless as one of those kitchen tools that specifically cuts egg into slices, but you could have done that with a knife.  earleybird 3 months ago  \n",
      "              \n",
      "\n",
      "I like that analogy.  I have one of those slicers and every time I use it I have two thoughts.  First, this slices so quick and evenly.  Second, there's a whole lot more I have to clean up compared to the knife.  thelastparadise 3 months ago  \n",
      "              \n",
      "\n",
      "I find the egg slicers to be a bit fragile. I'm on my 3rd one. The cutting wires keep breaking.  hospitalJail 3 months ago  \n",
      "              \n",
      "\n",
      "Does it introduce more lines of code? YesDoes it introduce features that you don't need to implement yourself? YesDoes it make it easy to drop OpenAI? YesWhat's the problem?  teaearlgraycold 3 months ago\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"You'll still need to re-tune your prompt and the hyperparameters when switching models. So the actual effort of switching models is not improved much if at all.  celestialcheese 3 months ago  \n",
      "              \n",
      "\n",
      "This.  I'll still use langchain for token/cost counting and some nice abstractions on top of the LLM, and the document loader system is semi-useful, but all of the retriever/chain stuff abstracts away the most important part - the prompt.  Tostino 3 months ago  \n",
      "              \n",
      "\n",
      "I've tried to switch out prompts between LLM's and I've had to change every bit of code provided by laying chain with a different implementation between them. It is an entirely useless abstraction. The prompt template is not at all transferrable.  redox99 3 months ago  \n",
      "              \n",
      "\n",
      "Hopefully as LLMs get smarter, you'll need less prompt engineering and stuff will \"just work\" across models.  aiunboxed 3 months ago\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"Thank god, there is someone else who feels the same way. There are tons of modules and none of them is production ready.It feels good for hobby / college projects, but would not use it for production.The pace at which they release features is also scary.  shepardrtc 3 months ago  \n",
      "              \n",
      "\n",
      "They also have no issues releasing stuff that is clearly not working yet.  But they got that sweet VC cash so yolo.  krawczstef 3 months ago\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"I had been hearing these pains from Langchain users for quite a while. Suffice to say I think:1. too many layers of OO abstractions are a liability in production contexts. I'm biased, but a more functional approach is a better way to model what's going on. It's easier to test, wrap a function with concerns, and therefore reason about.2. as fast as the field is moving, the layers of abstractions actually hurt your ability to customize without really diving into the details of the framework, or requiring you to step outside it -- in which case, why use it?Otherwise I definitely love the small amount of code you need to write to get an LLM application up with Langchain. However you read code more often than you write it, in which case this brevity is a trade-off. Would you prefer to reduce your time debugging a production outage? or building the application? There's no right answer, other than \"it depends\".To that end - we've come up with a post showing how one might use Hamilton\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"other than \"it depends\".To that end - we've come up with a post showing how one might use Hamilton (https://github.com/dagWorks-Inc/hamilton) to easily create a workflow to ingest data into a vector database that I think has a great production story. https://open.substack.com/pub/dagworks/p/building-a-maintain...Note: Hamilton can cover your MLOps as well as LLMOps needs; you'll invariably be connecting LLM applications with traditional data/ML pipelines because LLMs don't solve everything -- but that's a post for another day.  PaulHoule 3 months ago\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"Every system I’ve seen for managing this kind of system has flaws, including the ones that I have written.For instance scikit-learn implements excellent algorithms for model selection that would apply, in principle, to a model based on huggingface transformers that might take 2 hours to train.  skl is a fast machine if memory fits in RAM on a single computer, but it is not up to task for multiple computers or anything mortal to a single process such as the computer bring turned off.HF has model selection algorithms too, but not as nice.  They don’t take the same kind of datasets as all so it would be a hassle to import my ski models into HF.I have to be able to compare models generated with any kind of tools so I think I will build a universal model selection framework (builds and test models) but then you run into the problems langchain did where there is a lot of structure imposed and all sorts of quirks and performance losses because of that structure.For instance my current skl\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"all sorts of quirks and performance losses because of that structure.For instance my current skl selector wastes a lot of resources computing stuff from scratch over and over again and if the code were properly organized it could get the job done 3 times faster but the same trick wouldn’t work for every other experiment I might want to do.So we are all running into hurdles and finding ways to jump over them,  making a lot of mistakes because we are in a rush and don’t know better yet.  namuol 3 months ago\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"I tried to improve the type coverage of the JS package and I came to a lot of the same conclusions. It feels like a lot of unnecessary and poorly conceived indirection and abstraction. There’s basically just a lot more to learn especially if you’re just getting started.It’s maybe useful as a repository of prompting techniques, but I found myself constantly monitoring the actual prompt text being generated rather than reading the code that produces it…  zxienin 3 months ago  \n",
      "              \n",
      "\n",
      "My read on lot of justifiable criticism of langchain is, that it is rooted in trying not to lose out the lead it established since Nov 2022.  \n",
      "Pace of updates in LLM space have been staggering.Langchain hasn’t invested enough in quality of lib. A good lib needs good deliberation to build right abstractions that add value, then “get out of the way“.  rmonvfer 3 months ago\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"I've had a similar experience with LangChain. Initially, I was really impressed with it while developing an MVP, but as soon as I needed to add complexity or specific features, things started to go downhill.A significant chunk of my time was spent navigating through LangChain's codebase, trying to make sense of missing or outdated documentation. Plus, there's a significant disparity between the features of the JS/TS and Python versions. Identifying which version supports which features can be a real challenge.I don't want to sound overly critical, as I'm currently using it in production. However, I had to modify it so extensively that it barely resembles the original project anymore. Looking back, I can't help but think that adopting this library might have been a misstep. Perhaps we should have taken the time to create our own.  jpulec 3 months ago\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"I've been using the JS version of langchain for a few months now, and despite there being a lot of valid criticism, (especially around the abstractions it provides) I'm still glad to be using it.We get the benefits of a well used library, which means making certain changes is easy. For example, swapping our vector database was a one line change, as was swapping our cache provider. When OpenAI released GPT-4, we were able to change one parameter, and everything still just worked.Sure, it's moving fast, and could use a lot better documentation. At this point, I've probably read the entire source code several times over. But when we start testing performance of different models, or decide that we need to add persistent replayability to chains of LLM calls, it should be pretty easy. These things matter to production applications.  mmq 3 months ago\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"> When OpenAI released GPT-4, we were able to change one parameter, and everything still just worked.Wouldn't that be the same if you used the OAI js library directly? Basically swapping the model parameter?  jpulec 3 months ago  \n",
      "              \n",
      "\n",
      "It's not. The API is different, since GPT-4 is a chat based model, and davinci isn't. It's not a huge difference, but these little sort of things add up.  dbish 3 months ago  \n",
      "              \n",
      "\n",
      "It is a very minor change (made the changes in minutes and didn't have to bring in a new framework for it).  shepardrtc 3 months ago  \n",
      "              \n",
      "\n",
      "Agreed. It's a trivial change, certainly not a justification for using langchain.  mmq 3 months ago  \n",
      "              \n",
      "\n",
      "I see, thought you were using GPT-3.5 and moved to GPT-4.  sandGorgon 3 months ago\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"I think that Generative AI applications is a config management problem. Think Prompts X Chains X LLMs. Your prompts wont work across everything and everything will break on model change. Coding this into ur classes is what everyone does.I think the better answer is to declaratively pull out the prompts X chains as jsonnet code. Call it trauma & learnings from the K8s/Borg world. We have formats that have evolved as a result of millions of lines of code wrangling clusters/terraform/etc - so we decided to build a SDK over it.that is what we did here - https://github.com/arakoodev/EdgeChains/releases/tag/0.2.0EdgeChains is basically Generative AI prompt engineering modeled as config management.  cloudking 3 months ago  \n",
      "              \n",
      "\n",
      "What business problems are you solving with Langchain and LLMs?  wahnfrieden 3 months ago  \n",
      "              \n",
      "\n",
      "Reselling access to Langchain and LLMs  cloudking 3 months ago\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"Reselling access to Langchain and LLMs  cloudking 3 months ago  \n",
      "              \n",
      "\n",
      "Perfect.. I've seen so many demos showing people feeding data to LLMs so they can \"ask questions about their data\" but still not seen any real business use cases. Is anyone using these tools in production for a real problem?  ramoz 3 months ago\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"The enterprise search domain has evolved quite a bit, utilizing & integrating retrieval-rerank architectures for a few years now. That includes embedding/vector storage, as well as LLM usage (eg using Google T5 for query understanding and model rerank)… there’s some generative modeling but not really like GPT… more like structuring an abstract tree from a complex Boolean query, and sending that tree into an custom retrieval approach over ANN & rerank ensemble.If you’re doing that type of work in a serious manner you don’t use libraries like langchain or lamaindex. They are a bit late/irrelevant to the engineering that exists in that type of environment.  shon 3 months ago  \n",
      "              \n",
      "\n",
      "Lol  hcks 3 months ago  \n",
      "              \n",
      "\n",
      "It’s LLM wrappers all the way down  hamhamed 3 months ago\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"It’s LLM wrappers all the way down  hamhamed 3 months ago  \n",
      "              \n",
      "\n",
      "I tried using langchain.js after a bit of hype and overwhelming dev support and incremental releases, but it became added complexity for no reason for our case.OpenAI's NPM is much easier to use. A lot of what langchain promised out of the box didn't work like caching. Anyways it's still beta i believe  Der_Einzige 3 months ago  \n",
      "              \n",
      "\n",
      "This guy's analysis is terrible. I literally built a retrieval augmented generation with memory in langchain last night (comments claim you can't do this)Sure, langchain has poor documentation and useless helper functions. That doesn't mean that it's pointless.  tehsauce 3 months ago\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"Just fyi, if you use a standard knn library like faiss and pretty much any embedding + language model raw from huggingface or an API, it will require ~15 lines of code to do what you describe. I’m not sure how much shorter langchain made your implementation but I can’t imagine it saving too much.  risyachka 3 months ago  \n",
      "              \n",
      "\n",
      "The point is in 99% of use cases no one will ever swap OpenAI or vector storage db for anything else. All Langchain does in these cases is introduces useless abstraction that takes longer to implement and makes things less transparent.And when you need some customization instead of taking you 10 minutes it takes an hour to work around this abstraction.  chpmrc 3 months ago  \n",
      "              \n",
      "\n",
      "Could you please share the code? I tried doing the same and, like the people in that thread, could not. TIA!  jmugan 3 months ago\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"I found that you need to dig into the actual code and debug it while it is running to see what is going on to actually use the library (both with Langchain and LlamaIndex). That's unfortunate, but it does show a path to get you to where you want to go and is probably faster in the long run than writing your own code because you can swap components (e.g., which LLM or which vector DB) in and out.  holografix 3 months ago  \n",
      "              \n",
      "\n",
      "Langchain is an obvious VC / investor hustle by a handful of smart developers who are betting on the low sophistication of entrants to ML and Data science.It doesn’t do much but it wraps some obvious functionality with method names and paper thing abstractions that speaks the language of people who don’t know Python beyond Jupiter Notebooks.All power to them, they saw a gap and pounced.  tedtimbrell 3 months ago\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"For me the biggest benefit is just following the project and seeing what the community is doing with llms (It’s also not bad for quick proofs of concept).That said especially in python it’s not that hard to reimplement things yourself in a cleaner way. Output parsing for agents was nice but with the function update from OpenAI it’s not really necessary (if you’re just using their API)  moneywoes 3 months ago  \n",
      "              \n",
      "\n",
      "Didn’t they raise at a $200 million valuation  revskill 3 months ago  \n",
      "              \n",
      "\n",
      "I don't trust them, because they never talk about their \"tradeoffs\". Every software has its tradeoff.  politelemon 3 months ago  \n",
      "              \n",
      "\n",
      "I've rarely ever seen software talk about its tradeoffs. They want you to use the software.K8s and react are wildly popular, huge tradeoffs, but mention none of them.  shermix011 3 months ago  \n",
      "              \n",
      "\n",
      "DuckDB does on their page. https://duckdb.org/  anonzzzies 3 months ago\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"DuckDB does on their page. https://duckdb.org/  anonzzzies 3 months ago  \n",
      "              \n",
      "\n",
      "Everyone was so hyped up by this useless thing that I thought I was going crazy. Literally everything is done faster and easier and more readable and more maintainable doing it with a lines of Python yourself. Lot of hype with AI frameworks, especially this one.  dmezzetti 3 months ago  \n",
      "              \n",
      "\n",
      "If you'd like another option built on a foundation of open LLMs, open models and open source, check out txtai (https://github.com/neuml/txtai).Disclaimer: I am the author of txtai  DrDroop 3 months ago  \n",
      "              \n",
      "\n",
      "const THE_LEFT_HANDED_IDEAL_FOR_THE_EVEN_NUMBERS_OF_THE_NATURAL_NUMBERS_UNDER_MULTIPLICATION = 2  cdl 3 months ago  \n",
      "              \n",
      "\n",
      "IMO it's a great educational tool to understand how to build AI apps at least at a high level. It provides a good mental model. Can someone name another single resource that achieves the same?  mpaepper 3 months ago\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"I agree it's not so easy to work with Langchain.That's wy I built the simple LLM agents repo which you can understand in 5 minutes:https://github.com/mpaepper/llm_agents  lgrammel 3 months ago  \n",
      "              \n",
      "\n",
      "If you’re using JS/TS and want to have a nicer API for LLM calls (and a bit more), check out https://github.com/lgrammel/ai-utils.js  drewbitt 3 months ago  \n",
      "              \n",
      "\n",
      "Appreciate your work on this.  indus 3 months ago  \n",
      "              \n",
      "\n",
      "Langchain at times remind me of a Java API to send HTML from server to agent.HTMLElement root = new HTMLElement();\n",
      "// serialize as text and send root to output streamI was like why not send the string “<html>” to the output stream.  jmilldotdev 3 months ago  \n",
      "              \n",
      "\n",
      "Stop doing prompt ops.https://pbs.twimg.com/media/FzZxxmIacAEBPof?format=png&name=...  stan_kirdey 3 months ago\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"there is an alternative that is production-grade - deepset haystack https://haystack.deepset.ai/p.s. i am contributor so there could be bias  solomonb 3 months ago  \n",
      "              \n",
      "\n",
      "LangChain is a perfect example of unnecessary abstraction. You could build a much simpler and more composable library for working with LLMs simply using functions and function composition. This isn't rocket science.  szopa 3 months ago  \n",
      "              \n",
      "\n",
      "At one point I got so frustrated with langchain that I wrote my own stuff in go: https://github.com/ryszard/agency  jdprice 3 months ago  \n",
      "              \n",
      "\n",
      "Completely agree. I had this exact thought last week. It's been helpful to go through their repo to see how they've done certain things, but it's a textbook leaky abstraction.  tippytippytango 3 months ago  \n",
      "              \n",
      "\n",
      "It’s technical rent seeking and premature abstraction.  hybridrxn 3 months ago\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"It’s technical rent seeking and premature abstraction.  hybridrxn 3 months ago  \n",
      "              \n",
      "\n",
      "100% agree.  It is just a tactical way to SLOW LLM startups down with debugging hell.  Highly would not recommend.  zoba 3 months ago  \n",
      "              \n",
      "\n",
      "I want an off the shelf “talk to PDFs” solution and was hopeful langchain was it.  donpark 3 months ago  \n",
      "              \n",
      "\n",
      "I agree that LangChain is pointless for experienced ML developers building products. For the rest, I disagree as just getting to the point where same observation can be made is worthwhile.  ufo 3 months ago  \n",
      "              \n",
      "\n",
      "What is langchain?  noman-land 3 months ago  \n",
      "              \n",
      "\n",
      "I pasted your question directly into DDG and the entire first page has the answer.https://duckduckgo.com/?q=What+is+langchain  vegetax 3 months ago  \n",
      "              \n",
      "\n",
      "the time you spend to understand langchain, is much more than the time you make it yourself  0xPetra 3 months ago\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"LangChain is a \"utils\" libraryChange my mind.  handonam 3 months ago  \n",
      "              \n",
      "\n",
      "the consistency and conventionality of the methods in the langchain library are very sporadic, imo  shiva066 3 months ago  \n",
      "              \n",
      "\n",
      "shivashankar  siva7 3 months ago  \n",
      "              [–] \n",
      "\n",
      "It's easy to hate on langchain. Sure, the architecture isn't the best but it's where the crowd is so it's a safe bet  tikhonj 3 months ago  \n",
      "              [–] \n",
      "\n",
      "That just makes it easier to hate. Our collective tendency to build atop mediocre foundations just because they got popular first is exhausting.\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised Timeout: Request timed out: HTTPSConnectionPool(host='api.openai.com', port=443): Read timed out. (read timeout=600).\n",
      "Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised Timeout: Request timed out: HTTPSConnectionPool(host='api.openai.com', port=443): Read timed out. (read timeout=600).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new StuffDocumentsChain chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"Someone believes that Langchain is pointless.\n",
      "\n",
      "The Langchain platform is being criticized for attempting to solve problems that cannot be supported by its current technical foundations. The lack of reusability in the current generation of language models (LLMs) is a key issue, as each feature and step in the LLM chains requires custom handwritten prompts and specific data formatting. The setup of the DAG orchestration is a small part of the work, with the majority being in prompt tuning and data serialization. Langchain aims to address this lack of reusability by establishing abstractions for reuse.\n",
      "\n",
      "The author is criticizing Langchain for not effectively reusing resources and creating a framework that produces inefficient and unnecessary data. They mention that they attempted to create their own Langchain but ultimately found it unfeasible given the current foundational models.\n",
      "\n",
      "The commenter expresses frustration with LangChain's hidden prompts and the difficulty of changing them. They criticize the default prompt and suggest that other language models may not work as well as GPT-3.5 and GPT-4. The commenter also mentions their own successful use of a prompt they wrote six months ago. They then ask for more information about the \"highly sophisticated LLM chains\" used in LangChain and express curiosity about the challenges, process, and insights involved.\n",
      "\n",
      "The person is asking for examples or insights on how the prompts for generating next questions or condensing past questions have been improved to incorporate memory and provide better context for the user's input.\n",
      "\n",
      "The speaker is unsure about where memory is stored in langchain. They mention a specific insight regarding the MULTI_PROMPT_ROUTER_TEMPLATE in the langchain repository. They criticize the use of instructions for an LLM and suggest using more commonly used formats like markdown or type signatures instead of open json comments. They believe that too much reliance is placed on the intelligence of the LLM and that the prompt does not work well with GPT4All models. The speaker offers their own version of the prompt.\n",
      "\n",
      "The author finds the inclusion of the \"next_inputs\" parameter in JSON to be unnecessary and provides their simplified version of the code. They believe their version is more powerful because it does not require advanced intelligence, but instead meets their specific needs.\n",
      "\n",
      "The commenter was inspired to write a new section in their project, \"Prompts on the outside,\" after reading something.\n",
      "\n",
      "The author believes that current techniques in Natural Language Processing (NLP) for prompt engineering, such as token weighting and negative prompts, can be applied to regular language models. However, the NLP community has not developed the necessary tools for it yet. The future of prompt engineering in language models will involve specialized and engineered prompts, similar to the case with images in civit.ai. Consequently, many of the current prompts will become obsolete.\n",
      "\n",
      "The domain Automatic111 is a platform for desktop experimentation using Jupyter. However, when transitioning to production, there are additional complexities like prompt routing that need to be addressed. The concept of Generative AI is seen as a form of configuration management, modeled on the proven jsonnet grammar used in large production configurations. A trivial example can be found in a GitHub repository. This approach may be useful for those seeking a reality check on their AGI development.\n",
      "\n",
      "In a discussion about the limitations of open source models like GPT, a user shares their experience of using GPT professionally and encountering issues with quality and the need to re-tune prompts when upgrading to new OpenAI models. Another user suggests that the emergent behaviors that distinguish models like GPT and Gemini are a result of their large parameter count, which the open source community believes is unnecessary. A third user asks if the first user can share their lessons learned for others to benefit from.\n",
      "\n",
      "The discussion revolves around the release of an updated version of GPT (a language model) that disrupts custom prompts. The need for running language models locally is emphasized, both for maintaining intellectual property and preventing leaks to AI providers. The concern is raised that these updates might actually make the models less offensive. The question is posed regarding the interdependence of prompt iteration and the DAG (Directed Acyclic Graph) phase, with a desire to involve non-technical colleagues in modifying prompts. The extent to which prompts need to evolve alongside the DAG is unclear.\n",
      "\n",
      "The author discusses the challenges of prompt customization in various language models. They mention that generating the next step in a sequence is difficult and often leads to incomplete or repetitive outputs. As a result, they have stopped attempting this and instead provide a narrow use case with a predefined structure. They also mention that tuning prompts is a sensitive process.\n",
      "\n",
      "The author explains that tuning prompts within a team is important, as specific words and even newlines can greatly affect quality. They mention that it is doable for non-tech individuals, and sometimes expose it to users for additional context. However, they state that it is difficult to get it right and requires extensive testing and manual evaluation. The author concludes that current tools like Langchain are not useful for building valuable user-facing features at a production scale.\n",
      "\n",
      "The suggestion is to create a scoring system for re-usable action sequences stored in a database and have an AI generate alternative solutions and grade them based on their performance. The action sequences can be evaluated based on success, speed, efficiency, cleverness, and cost. Human feedback can also be incorporated, and people can be rewarded for proposing successful and efficient action sequences. The AI can query the database to find effective action sequences to combine. Investing more in generating and improving action sequences in the database can make the AI agent smarter and more effective.\n",
      "\n",
      "The requestor expresses interest in an open-source version of the internal Langchain that was built, wanting to know the architectural improvements made in the use-case.\n",
      "\n",
      "The individual encountered a problem with the lack of reusability in Generative AI and attempted to solve it with Edgechains. They believe that Generative AI is a configuration management problem and propose using a config management grammar, specifically jsonnet, to model it. They are seeking examples of complex chains that could be solved using Edgechains-jsonnet.\n",
      "\n",
      "The speaker found the commenter's insights interesting and wants to know what they do. The speaker is a researcher at FAANG and is starting a new company in the same field. They would like to connect and exchange emails. The commenter asks about the integration pattern the speaker uses.\n",
      "\n",
      "The summary states that the author has a blog post criticizing LangChain and has created a simple alternative. They found LangChain difficult to work with and concluded that it was easier to create their own Python package. The author points out that LangChain promotes tool lock-in without much benefit for developers and has suboptimal implementations. Debugging LangChain errors is difficult and hacking it for customization is also challenging. The documentation is lacking important details.\n",
      "\n",
      "The summary is about a person's concern regarding the lack of documentation and transparency in the use of custom agents in hacking. They also express worry about the influence of LangChain on the AI ecosystem, which is hindering development and code clarity. The person is hesitant to criticize open source software but feels the need to address these issues.\n",
      "\n",
      "The author is hesitant to release a blog post criticizing an open source software called LangChain because they don't want to be seen as an antagonist. However, they believe that the software's growing popularity and significant investment indicate that it is no longer a small volunteer project and therefore should be subject to informed criticism.\n",
      "\n",
      "The author is expressing concern about LangChain's ability to handle criticism, specifically citing fanboys who defend the platform and dismiss any negative feedback. They mention not having the time to deal with such messages and express a similar worry about building Auto-GPT. Another commenter suggests ignoring the fanboy messages.\n",
      "\n",
      "The discussion is about the difficulty of debugging LangChain errors and the lack of basic functionality for seeing what is happening within the framework. One person suggests creating a custom interface for logging queries.\n",
      "\n",
      "The speaker acknowledges concerns and questions regarding their decision not to use Auto-GPT for large complex projects. They appreciate feedback but emphasize that Auto-GPT is not suitable for their specific needs and do not want to falsely endorse it.\n",
      "\n",
      "This post praises LangChain while criticizing those who slander it. It argues that sometimes popular projects can rise above criticism and mentions examples like React, Kubernetes, and Tailwind.\n",
      "\n",
      "This passage discusses the merging of tactics from different ecosystems into a simplified and intuitive chart. It mentions a critic who fails to see the value in this approach and believes the developers are either lost or foolish. The passage also references cognitive dissonance and the critic's belief that they are the only sane one in a mad world. The summary suggests that there will be a climax or revelation to come.\n",
      "\n",
      "The author expresses their excitement about a potential event that could lead to madness and a surprising revelation. They mention the use of an Irish pirate persona to soften their critique of another's perspective. They also acknowledge the positive impact of promoting LangChain to a wider audience.\n",
      "\n",
      "The person is hesitant to release a blog post criticizing open source software, as they don't want to be seen as someone who criticizes software that is operating in good faith. However, another person encourages them to release it to warn others before they invest in the software.\n",
      "\n",
      "The speaker's attempt to understand langchain has sparked their motivation to create it themselves.\n",
      "\n",
      "The person agrees that they do not like LangChain abstractions as they are not truly composable and require a lot of time to understand. They believe that LangChain's code seems rushed and is not a good long-term framework. They have developed their own alternative called LiteChain, which they believe is a better option as it has actual composable monads and a simpler code structure.\n",
      "\n",
      "The person does not see a need for anything beyond an ETL (extract, transform, load) process and believes that there are already robust tools available for that purpose. They are more interested in prompt development and connecting prompts together, and suggest the need for an \"LLM framework\" or a small library to solve that instead of an ETL process. However, they admit to not being experienced with ETLs and need to explore the available tools to see if they can achieve their desired goals.\n",
      "\n",
      "The speaker had a conversation with GPT-4 and it was not convinced that ETLs (Extract, Transform, Load) would be effective for the same purposes as LiteChain. The speaker would like to see more examples of LLM application building on ETLs, such as the video shared by another user. The second user agrees and mentions their involvement in a project called Hamilton.\n",
      "\n",
      "The author questions the need for LlamaIndex and LangChain as they believe existing technology, such as Spark, is already robust for ETL processes. They mention Prophecy working with large enterprises for generative AI use cases. The author provides links to a talk and a blog/demo related to Prophecy's generative AI platform.\n",
      "\n",
      "The speaker mentions working on platform and customer projects that involve pipelines to feed data to louie.ai or real-time headless versions. They believe that using LLM is mostly beneficial for vendor neutrality. They reference Apache Nifi or Spark Streaming as examples of similar technologies. They state that their more challenging work involves data engineering, but they also work on more complex scenarios for louie.ai where structured programming abstractions are important for AI-style business logic. This involves generating on-the-fly analytics with interactive data visualization.\n",
      "\n",
      "The person agrees that using langchain can be silly, but they primarily use it to simplify the process of working with BYO LLMs and extracting information from documents. They find it convenient for quickly prototyping ideas and swapping out language models. However, they also note that langchain is inefficient and often encounter bugs and performance issues, leading them to re-implement the necessary components themselves. Another commenter suggests that using low-code platforms like Prophecy.io can be faster for prototyping, as they offer built-in connectors for various tasks.\n",
      "\n",
      "The person is expressing their opinion that they have not found value in using langchain. They mention that it adds unnecessary abstractions and does not provide any significant advantages over simpler methods like using functions and f strings. They acknowledge that other people may like it, but they personally do not want to depend on a large library with little value added to their codebase.\n",
      "\n",
      "The author is questioning the usefulness of prompt templates in the programming language tool LangChain compared to using a simple function and f string. They express frustration with the language dependency and consider if they can achieve their needs in their preferred environment. They also compare the chaining of \"DAGs\" in LangChain to a sequence of function calls in a loop.\n",
      "\n",
      "The project being discussed in the comment is seen as only useful for providing a list of AI tools and examples of using their APIs. The commenter suggests that coding directly or using a more complex dataflow framework would be more efficient for other aspects of the project.\n",
      "\n",
      "The comment discusses the value of a code snippet library, suggesting it may have worked better as a documentation site. It also mentions confusion regarding plain old sequential code and recommends a Python dependency tool. The commenter questions whether people genuinely like the platform or if the LangChain team has been successful in promoting it, mentioning an interview with Andrew Ng as a possible result of active outreach.\n",
      "\n",
      "LangChain, a company that offers standardized and swappable templates for marketing, raised $10 million in a seed round and has been successful in maintaining the perception of value. However, there are doubts about the effectiveness of their product. They gained popularity after the hype around ChatGPT and have a low barrier for contribution.\n",
      "\n",
      "The LangChain ecosystem was found to be useful initially, but the abstractions were not helpful and the user had to create their own. They then started using jinja templates, finding value in the syntax and ability to render prompts using dictionaries. Another user agreed with this sentiment and mentioned their work on AIPL, a simple scripting language with array semantics and inline prompt construction. The user also mentioned the ability to switch to Python when needed.\n",
      "\n",
      "The author questions the need to learn a new programming language for tasks that can easily be accomplished in existing languages like Python or JavaScript. They believe that most tasks in LLM pipelines, such as string manipulation, API calls, and storage, are not complex and can be handled with the standard library. Additionally, they mention that there are already established solutions available for more complex cases. The author wonders what the added value is of learning a new language and suggests they may not be the target audience for such languages.\n",
      "\n",
      "The speaker questions whether they are the intended audience and wonders who the target audience might be for a complex task related to LLM work. They express uncertainty about the value of statistics analysis but find the value of chaining responses unclear.\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new StuffDocumentsChain chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"The speaker has been building their own tooling for LLM projects, which covers a portion of Langchain's functionality. They had initially thought their work was wasted when they heard about Langchain, but upon using it, they found it to be poorly performant and produce worse results. They advise against using Langchain without carefully considering its purpose and not just using it as a messy, glue code solution.\n",
      "\n",
      "The person has written thousands of lines of code for a data cleaning pipeline but has struggled to replace parts of it to use a local language model (LLM) instead of OpenAI. The chat-based templates they are using do not allow them to assign proper user/assistant names, resulting in poor performance for the local LLM. They find the templates to be a useless abstraction and plan to remove them from their code as soon as they find a better alternative.\n",
      "\n",
      "The commenters are discussing different attempts at generative AI in the domain of configuration management. One project uses jsonnet, a proven grammar used in large scale Kubernetes, while another project uses plain JSON. The commenters express interest in these projects and plan to check them out.\n",
      "\n",
      "The commenter had a similar experience and did not find much benefit in moving to something better. They question if they are missing something and believe the added complexity is not worth it. Another commenter mentions that the project had raised a significant amount of money and refers to a controversial discussion about software quality.\n",
      "\n",
      "The comment acknowledges that companies can still succeed even with technical issues or inefficient practices if they have a strong sales team and a desired product. However, the comment suggests that the conversation is more focused on overeager venture capitalists and a product that may not be necessary, rather than technical difficulties. The commenter admits to not having enough experience with the product to support these claims.\n",
      "\n",
      "This discussion revolves around the decline of Sequoia in the venture capital world and its struggles in the enterprise sector. It is compared to the fall of another top player, KPCB, in the past. The rumored valuation of a product associated with Sequoia is considered to be excessive given its lack of revenue.\n",
      "\n",
      "The discussion revolves around who is most negatively affected in a situation where a company, Langchain, fails to meet revenue targets and needs to pivot. One commenter suggests that investors may be the ones who suffer the most if they overvalue the company and cannot recoup their investments. The other commenter adds that while founders and early employees may receive a decent salary, investors are at higher risk as they rely on the company's success for a return on their investment.\n",
      "\n",
      "The product in question is not specified, but both commenters are in agreement about its identity.\n",
      "\n",
      "The use of LLM frameworks at this time is not practical and may be harmful. There are ongoing research and exploration in extracting value from LLMs, including methods such as prompting and memory management. These developments are still in the early stages and not yet ready for implementation. It is important to remain adaptable and open to new ideas as the field continues to evolve. Choosing a framework at this stage may limit flexibility and hinder progress.\n",
      "\n",
      "The speaker believes that trying to establish stability in difficult times will come at a cost, and will cause them to fall behind. This is their personal perspective.\n",
      "\n",
      "The person is requesting resources to learn about various topics related to prompting, including chain of thought, react, reflection, tree of thoughts, zero vs few hot, completion vs conversational interfacing, memory management via vector databases, and prompt expansion vs compression vs progressive summarization.\n",
      "\n",
      "The summary is about various resources related to artificial intelligence and natural language processing techniques. It includes links to research papers such as \"Chain of thought,\" \"ReAct,\" \"Reflexion,\" and \"Tree of thoughts.\" There is also a video that provides an overview and comparison of these methods. The discussion mentions the OpenAI API documentation and the concept of completion vs conversational interface. Lastly, there is a comment expressing criticism towards the abstraction and limitations of the mentioned methods.\n",
      "\n",
      "The term \"Fauxcade pattern\" is being referenced, which is a portmanteau. It already exists and is defined on Urban Dictionary. The source was provided by a user named teaearlgraycold three months ago.\n",
      "\n",
      "The author has researched why people use langchain and identified two main reasons. Firstly, langchain provides beginner-friendly documentation that helps newbies understand the concepts and feel accomplished. Secondly, langchain allows users to easily swap different components and measure the results, making it unnecessary to complicate back end code.\n",
      "\n",
      "The speaker has created their own abstraction layer to achieve a desired outcome with minimal code, which they believe is appropriate for a Python notebook.\n",
      "\n",
      "A person expresses concern about their langchain experience on their resume and is advised to focus on their actual skills and sustainable, evergreen skills. It is revealed that langchain is a joke and the person is relieved.\n",
      "\n",
      "The person initially thought the comment was sarcastic and targeting the conceit of a Reddit post. They suggest that the library being called pointless is not a big deal because it is still new. The downvotes on their post make them think that the joke is better than they initially thought. The context for the joke is that some job listings have unrealistic requirements. Someone else responds with \"Whooosh.\"\n",
      "\n",
      "The commenter believes that wrapper libraries and SDKs, such as Langchain, are often unnecessary and can be easily replaced with string manipulation and HTTP requests. They also criticize ORMs for making false promises about easily swapping out underlying components. They suggest that Langchain may not be needed.\n",
      "\n",
      "The question being discussed is difficult to answer and requires careful analysis. The behavior of the OpenAI API has been described as bizarre and almost fantastical. Langchain is not very helpful in understanding this. Despite being several months into GPT-4, most people have either lost interest or are hastily building prototypes in Python.\n",
      "\n",
      "The author believes that the abstractions in Langchain are flawed because they restrict modification and experimentation within the chain. They propose that a better abstraction would be a DAG or state machine, which would provide more control over the distinct stages in the pipeline. The author acknowledges that fixing this issue would be challenging and require significant reworking.\n",
      "\n",
      "Fixing a fundamental issue in the system would require a complete overhaul of the library, which is a difficult task.\n",
      "\n",
      "Promptfile and EdgeChains are alternative approaches to prototyping and app flow that offer different features than traditional methods. Promptfile uses markdown, while EdgeChains uses jsonnet for configuration management. Additionally, the comment mentions that a Directed Acyclic Graph (DAG) may not be expressive enough for certain applications and suggests using a Lang chain instead.\n",
      "\n",
      "Both commenters express skepticism about the usefulness of langchain, stating that they found it more efficient to write their own code instead. They suggest that the abstractions provided by langchain may be helpful for inexperienced developers in the short term.\n",
      "\n",
      "The co-founder of LangChain acknowledges and appreciates the feedback they have received regarding their platform. They are working hard to address the feedback and make it easier to build LLM applications. They value feedback and try to respond and address it as quickly as possible, although they may not be able to respond to all feedback directly. They have been working on improving their documentation and have made some changes, but there is still more to be done. They would appreciate specific feedback on which chains or components need better documentation.\n",
      "\n",
      "The feedback on which chains/components require better documentation is valuable. The company aims to improve the customizability of prompts, chains, and agents by making it easier for users to customize them through better documentation and modular components. Additionally, they are developing separate tooling to address general difficulties in building LLM applications, such as debugging and testing. This tooling is expected to be launched soon.\n",
      "\n",
      "The writer believes that Langchain is a superior tool for developers, despite the criticisms it receives. They find it easy to use and appreciate its ability to switch between different datastores and embeddings effortlessly. They also note that Langchain simplifies the process of loading files into a vector store. While they initially found it challenging to get started with Langchain, they attribute this to a lack of understanding of the ecosystem rather than any flaws in the tool itself. The writer mentions that the documentation has improved significantly.\n",
      "\n",
      "The author appreciates the efforts made to improve the documentation, but requests that certain information be shielded without limiting choice. They encourage the recipient to not let negative feedback discourage them.\n",
      "\n",
      "The author acknowledges the challenges and flaws of using langchain in Louie.ai but also highlights its benefits in promoting pluggability and avoiding vendor lock-in. They mention the need to work around langchain's limitations in areas such as synchronization and multithreading. The author also mentions building custom libraries and not using codegen capabilities for security reasons. They conclude the statement by mentioning job opportunities at Louie.ai and Graphistry.com.\n",
      "\n",
      "The commenter finds the thread title unintentionally funny and expresses a desire for a more functional interface in order to write \"point-free\" pipelines, particularly in relation to memory and multithreading. They mention that LangChain is winning as a pluggable interface for many new LLM providers. However, another commenter expresses frustration with LangChain, finding their abstractions and documentation unhelpful and incomplete, suggesting that LangChain has not yet made LLM app development easier.\n",
      "\n",
      "Several users have shared their opinions on LangChain, a tool for creating prompt templates and building Python libraries. One user found it useful for creating their own templates and learning about Python libraries, viewing it more as a sample codebase. However, other users found it pointless and counterproductive, stating that the templates were inconsistent and incompatible with different LLM backends. They suggested avoiding the library altogether and found it to be unnecessary for their LLM-based projects.\n",
      "\n",
      "The author compares the current situation in data science to the past situation in frontend development, where people would indiscriminately install unnecessary packages. They believe that lessons will be learned in data science as well and express concerns about compatibility issues for Langchain due to their reliance on multiple abstractions.\n",
      "\n",
      "The problem highlighted is that programmers are accustomed to using code, but GPT-4 is designed to process semantics without the need for code. However, programmers still try to improve the system using code, even though templates and f-strings can often solve the problem. In some cases, there may still be programming required for integration into existing systems or handling large amounts of data, but there are established solutions available.\n",
      "\n",
      "The Hacker News discussion on Langchain reveals that many commenters believe the platform adds unnecessary complexity and does not offer significant benefits. The poor documentation, lack of customizability, and difficulty debugging are also criticized, along with the rapid updates. However, some appreciate the ability to swap components and see potential future improvements. Overall, sentiment is negative towards using Langchain for real production systems, as many prefer coding LLM workflows directly. However, its popularity makes it a safe initial choice for building.\n",
      "\n",
      "The discussion suggests that while Langchain has some architectural issues, its popularity and potential for prototyping and education make it a safe choice. However, overall, there is criticism of its technical merit and readiness for production.\n",
      "\n",
      "This conversation is discussing the unnecessary complexity and bloat of certain tools or patterns, using an analogy of a kitchen tool that cuts eggs into slices. Some commenters argue that these tools add unnecessary layers and complications, while others defend them for providing convenience and saving time.\n",
      "\n",
      "The commenters express that switching models still requires effort in retuning prompts and hyperparameters. They find the abstraction of the retriever/chain components less important compared to the prompt. One commenter mentions the difficulty in transferring prompt templates between different language models. However, there is hope that as language models improve, less prompt engineering will be needed.\n",
      "\n",
      "The commenters express relief at finding someone who shares their opinion that the available modules are not suitable for production. They also criticize the frequent release of new features, which they believe are often not functioning properly. They speculate that the company is able to release unfinished products due to receiving ample venture capital funding.\n",
      "\n",
      "The author believes that using too many layers of object-oriented abstractions in production contexts is a disadvantage and a more functional approach is better. They also argue that the layers of abstractions can hinder customization and make it necessary to step outside the framework. The author acknowledges the ease of coding with Langchain but emphasizes the importance of considering the trade-off between brevity and debugging in production. They introduce a post demonstrating the use of Hamilton as an alternative.\n",
      "\n",
      "The author suggests using Hamilton to create a workflow for ingesting data into a vector database. Hamilton is capable of covering MLOps and LLMOps needs, and can connect LLM applications with traditional data/ML pipelines.\n",
      "\n",
      "The author has found flaws in existing systems for managing a particular kind of system, including those they have personally written. They mention that scikit-learn has good algorithms for model selection but is limited in scalability, while huggingface transformers have less advanced model selection algorithms and importing models between the two systems is inconvenient. The author plans to build a universal model selection framework but acknowledges potential issues with imposed structure and performance losses. They mention their current use of scikit-learn.\n",
      "\n",
      "The speaker is describing how the current structure of their code is causing inefficiencies and performance issues. They believe that proper organization could significantly improve the speed of their code. However, they acknowledge that there are different challenges and experiments that may require different approaches. They also mention making mistakes due to rushing and lack of knowledge.\n",
      "\n",
      "The two commenters are expressing their dissatisfaction with a JavaScript package, citing issues with unnecessary complexity, a steep learning curve, and a lack of quality in the library. They suggest that the package may be focused on maintaining its market position rather than prioritizing quality and effective use of abstractions.\n",
      "\n",
      "The individual had a positive initial experience while using LangChain for their minimum viable product (MVP) development. However, they encountered difficulties when trying to add complexity or specific features, as they had to navigate through unclear or outdated documentation in the codebase. They also found a significant difference between the features offered in the JavaScript/TypeScript and Python versions, making it challenging to identify compatibility. Despite using LangChain in production, they had to extensively modify it, leading them to question whether using the library was a mistake and if they should have developed their own solution.\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new StuffDocumentsChain chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"The user has been using the JS version of langchain for a few months and acknowledges valid criticisms, particularly regarding its abstractions. However, they are pleased with the benefits of using a widely used library, such as easy changes to the vector database and cache provider. They mention the ease of adapting to OpenAI's GPT-4 release. The user acknowledges the fast pace and lack of documentation but believes it will be easy to test performance and add replayability to LLM calls in production applications.\n",
      "\n",
      "The discussion revolves around the change from using GPT-3.5 to GPT-4 in the OpenAI library. While some users believe that the change is minor and can be easily implemented, others point out that there are differences between the two models, particularly in terms of being chat-based. Overall, the consensus is that the change is not significant enough to warrant the use of a new framework like langchain.\n",
      "\n",
      "The person is discussing the use of Generative AI applications and how they can be seen as a configuration management problem. They suggest using Prompts X Chains X LLMs and JSONnet code to address the issue. They also mention a SDK called EdgeChains that is designed for Generative AI prompt engineering. Additionally, they mention that Langchain and LLMs are being used for reselling access and solving business problems.\n",
      "\n",
      "The person is asking if anyone is using Langchain and LLMs tools in production for real business problems, as they have only seen demos but no actual use cases.\n",
      "\n",
      "The enterprise search domain has undergone significant changes and now incorporates retrieval-rerank architectures, embedding/vector storage, and the use of LLM (such as Google T5) for query understanding and model reranking. Generative modeling is not prevalent, but rather the focus is on structuring abstract trees from complex Boolean queries and utilizing custom retrieval approaches with ANN and rerank ensemble. Libraries like langchain or lamaindex are considered late and irrelevant in this type of engineering environment, where LLM wrappers are used extensively.\n",
      "\n",
      "The first commenter tried using langchain.js but found it to be overly complex and lacking promised features. The second commenter disagrees and claims to have successfully built a retrieval augmented generation with memory using langchain. They acknowledge that the documentation and helper functions are not helpful but still find value in the tool.\n",
      "\n",
      "The commenter suggests that using standard libraries like faiss or huggingface's language models can accomplish what the original poster is describing in about 15 lines of code. They argue that using a platform like Langchain adds unnecessary complexity and makes customization more difficult. Another user asks for the code to achieve the same functionality.\n",
      "\n",
      "The comment discusses the need to debug the code while using the Langchain and LlamaIndex libraries, but acknowledges that it provides a quicker option compared to writing one's own code. The comment also suggests that Langchain may be targeted towards individuals with limited knowledge in Python and ML, highlighting its simplicity and use of familiar language.\n",
      "\n",
      "The comments provide different perspectives on the benefits and concerns related to using certain software. One person finds value in following a project and observing the community's activities, while another suggests that it is not difficult to improve upon existing software in a cleaner way. The issue of software tradeoffs is also raised, with one person expressing mistrust towards a company that does not openly discuss its tradeoffs. However, another comment mentions that many popular software tools do not explicitly address their tradeoffs, unlike DuckDB, which does.\n",
      "\n",
      "The comment thread discusses opinions on DuckDB, an AI framework, and offers an alternative option called txtai. The usefulness and hype around DuckDB are questioned, with some users suggesting that building AI apps with Python is faster and more maintainable. Txtai is presented as an option built on open LLMs, models, and source. One user considers DuckDB a valuable educational tool for understanding AI app development.\n",
      "\n",
      "The commenter agrees that working with Langchain is not easy. They provide a link to a repository where they have built a simple LLM agents repo that can be understood in 5 minutes. They also suggest another repository for JS/TS users who want a better API for LLM calls. Another commenter appreciates the work done. One commenter compares Langchain to a Java API for sending HTML from server to agent. Another commenter suggests to stop doing prompt operations and includes a screenshot link.\n",
      "\n",
      "The commenters are criticizing LangChain, stating that it is unnecessary, overly complex, and a leaky abstraction. One commenter even created their own solution in Go. They believe that a simpler and more composable library could be built using functions and function composition. Another commenter refers to LangChain as technical rent seeking and premature abstraction.\n",
      "\n",
      "The comments discuss various opinions on LangChain, with some users expressing dissatisfaction and recommending against its use. One user is hopeful that LangChain can provide an off-the-shelf solution for their needs. Another user believes that LangChain is not beneficial for experienced ML developers building products, but may be useful for others. One user asks for an explanation of what LangChain is, and another user suggests searching for the answer online. Lastly, a user states that it takes more time to understand LangChain than to create it oneself.\n",
      "\n",
      "The LangChain library is seen as inconsistent and lacking in conventionality by some users. Despite its flaws, it remains popular due to its widespread use and familiarity. However, this tendency to build upon popular but mediocre foundations is frustrating to some individuals.\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"The Langchain platform is being criticized for its lack of reusability in language models and the difficulty in customizing prompts. Some users have created their own alternatives to Langchain due to its limitations and lack of documentation. The effectiveness and value of Langchain are questioned, with suggestions for improvement and alternative approaches. The discussion highlights the challenges and concerns surrounding prompt engineering and the future of language models. Some users express frustration with Langchain's abstractions and its impact on the AI ecosystem. Overall, the consensus is that Langchain has shortcomings and may not be the most efficient tool for building valuable user-facing features.\n",
      "\n",
      "The discussion revolves around the use and effectiveness of Langchain, a tool for building local language model (LLM) projects. Some users have found Langchain to be poorly performant and produce worse results compared to their own custom tools. Others have criticized the documentation, abstractions, and lack of customizability in Langchain. However, there are also users who appreciate Langchain for its ease of use and potential for prototyping. There is skepticism about the usefulness of LLM frameworks in general, as they are still in the early stages of development. The discussion also touches on the challenges and risks of venture capital funding, the decline of certain companies in the venture capital world, and the importance of sustainable and evergreen skills. There are requests for resources related to prompting techniques and discussions about alternative approaches to configuration management. Overall, sentiment towards Langchain is mixed, with some praising its benefits and others criticizing its limitations.\n",
      "\n",
      "The discussion revolves around the use of LangChain, a JavaScript library for Generative AI applications. Some users acknowledge its shortcomings and complexity, while others find value in its features. The conversation also touches on the use of GPT-4 and the need for configuration management in Generative AI. However, some users believe that LangChain is unnecessary and suggest using other libraries or building custom solutions. There are mixed opinions on LangChain's usefulness and suitability for different user types. Overall, there is frustration with the tendency to rely on popular but mediocre software.\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"The Langchain platform is being criticized for its lack of reusability and customization options, with some users creating their own alternatives. There are concerns about Langchain's effectiveness and value, as well as frustrations with its abstractions and impact on the AI ecosystem. While some users appreciate its ease of use, there is skepticism about the usefulness of local language model frameworks in general. The discussion also touches on venture capital funding, decline of certain companies, and the importance of sustainable skills. Overall, sentiment towards Langchain is mixed, with praise for its benefits and criticism for its limitations.\""
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain = load_summarize_chain(llm=model,chain_type=\"map_reduce\", verbose=True)\n",
    "chain.run(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llms",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
