{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import PromptTemplate, ChatPromptTemplate\n",
    "from langchain.schema import StrOutputParser\n",
    "from operator import itemgetter\n",
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "from langchain.callbacks import get_openai_callback\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt1 = PromptTemplate.from_template(\"what is the city {person} is from?\")\n",
    "prompt2 = PromptTemplate.from_template(\"what country is the city {city} in? respond in {language}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Thành phố mà Donald Trump đến từ là New York, tiểu bang New York, thuộc nước Mỹ.'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain1 = prompt1 | model | StrOutputParser()\n",
    "chain2 = {\"city\": chain1, \"language\": itemgetter(\"language\") } | prompt2 | model | StrOutputParser()\n",
    "chain2.invoke({\"person\": \"Donald Trump\", \"language\": \"Vietnamese\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "prompt3 = PromptTemplate.from_template(\"generate a {attribute} color with the {intensity} saturation. Return the name of the color and nothing else.\")\n",
    "prompt4 = PromptTemplate.from_template(\"what is the fruit of color: {color}. Return the name of the fruit and nothing else.\")\n",
    "prompt5 = PromptTemplate.from_template(\"what is the name of the country with a flag that has the color: {color}. Return the name of the country and nothing else.\")\n",
    "prompt6 = PromptTemplate.from_template(\"what is the color of the {fruit} and the flag of {country}\")\n",
    "\n",
    "model_parser = model | StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StringPromptValue(text='what is the color of the Cherry and the flag of Kazakhstan')"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "color_generator = {\"attribute\": RunnablePassthrough()} | prompt3 | {\"color\": model_parser}\n",
    "# we can use itemgetter instead of RunnablePassthrough\n",
    "# color_generator = {\"attribute\": itemgetter(\"attribute\")} | prompt3 | {\"color\": model_parser}\n",
    "color_to_fruit = prompt4 | model_parser\n",
    "color_to_country = prompt5 | model_parser\n",
    "question_generator = color_generator | {\"fruit\": color_to_fruit, \"country\": color_to_country} | prompt6\n",
    "# question_generator.invoke({\"attribute\": \"warm\"})\n",
    "question_generator.invoke(\"warm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='The color of a cherry is typically red. \\n\\nThe flag of Chile consists of two horizontal bands of white and red with a blue square in the top-left corner. Inside the blue square, there is a five-pointed white star.')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = question_generator.invoke(\"warm\")\n",
    "model.invoke(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'llm1': 'completion', 'llm2': 'completion', 'total_chars': 20}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" This is example of RunnablePassThrough\"\"\"\n",
    "\n",
    "def fake_llm(prompt: str) -> str: # Fake LLM for the example\n",
    "    return \"completion\"\n",
    "\n",
    "runnable = {\n",
    "    'llm1':  fake_llm,\n",
    "    'llm2':  fake_llm,\n",
    "} | RunnablePassthrough.assign(\n",
    "    total_chars=lambda inputs: len(inputs['llm1'] + inputs['llm2']))\n",
    "\n",
    "runnable.invoke('hello')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Branching & Merging\n",
    "You may want the output of one component to be processed by 2 or more other components. RunnableMaps let you split or fork the chain so multiple components can process the input in parallel. Later, other components can join or merge the results to synthesize a final response. This type of chain creates a computation graph that looks like the following:\n",
    "\n",
    "     Input\n",
    "      / \\\n",
    "     /   \\\n",
    " Branch1 Branch2\n",
    "     \\   /\n",
    "      \\ /\n",
    "    Combine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_input = ChatPromptTemplate.from_template(\"generate an argument about: {input}\") | model | StrOutputParser() | {\"base_response\": RunnablePassthrough()}\n",
    "positive = ChatPromptTemplate.from_template(\"generate a positive aspects of {base_response}\") | model | StrOutputParser()\n",
    "negative = ChatPromptTemplate.from_template(\"generate a negative aspects of {base_response}\") | model | StrOutputParser()\n",
    "combined = ChatPromptTemplate.from_messages([(\"ai\", \"{original_response}\"), (\"human\", \"Pros:\\n{result_positive}\\n\\nCons:\\n{result_negative}\"),\n",
    "                                              \"system\", \"Generate a final response given the critique\"]) | model | StrOutputParser()\n",
    "chain = initial_input | {\"result_positive\": positive, \"result_negative\": negative, \"original_response\": itemgetter(\"base_response\")} | combined\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with get_openai_callback() as callback:\n",
    "    for s in chain.stream({\"input\": \"\"\"investing tremendous amount of time into learning and practicing \n",
    "                           an 10-month old framework called LangChain that enables building AI-powered apps fast and easy.\"\"\"}):\n",
    "        print(s, end=\"\")\n",
    "    print(f\"\\nHere is the cost breakdown for this call:\\n{callback}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with get_openai_callback() as callback:\n",
    "    result = chain.invoke({\"input\": \"\"\"investing tremendous amount of time into learning and practicing \n",
    "                           an 10-month old framework called LangChain that enables building AI-powered apps fast and easy.\"\"\"})\n",
    "    print(f\"\\nHere is the cost breakdown for this call:\\n{callback}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llms",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
