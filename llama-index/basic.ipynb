{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index import SimpleDirectoryReader, VectorStoreIndex, Document, SummaryIndex, StorageContext, load_index_from_storage, ServiceContext, set_global_service_context\n",
    "from llama_index.llms import OpenAI\n",
    "from llama_index.node_parser import SimpleNodeParser\n",
    "from llama_index import (VectorStoreIndex, get_response_synthesizer)\n",
    "from llama_index.retrievers import VectorIndexRetriever\n",
    "from llama_index.query_engine import RetrieverQueryEngine\n",
    "from llama_index.indices.postprocessor import SimilarityPostprocessor\n",
    "# import logging, sys\n",
    "# import openai"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Enabling Debugging Mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# logging.basicConfig(stream=sys.stdout, level=logging.DEBUG)\n",
    "# logging.getLogger().addHandler(logging.StreamHandler(stream=sys.stdout))\n",
    "# openai.log=\"debug\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### One-Time Setup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading the document, setting up the service context, llm to use, chunk size, chunk overlap\n",
    "documents = SimpleDirectoryReader('./data').load_data()\n",
    "llm = OpenAI(model=\"gpt-3.5-turbo\", temperature=0.8)\n",
    "#build the nodes\n",
    "parser = SimpleNodeParser.from_defaults(chunk_size=1024, chunk_overlap=24)\n",
    "nodes = parser.get_nodes_from_documents(documents)\n",
    "service_context= ServiceContext.from_defaults(llm=llm, chunk_size=1024, chunk_overlap=20, node_parser=parser)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "set up the global service once and for all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_global_service_context(service_context)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using another LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.llms import PaLM\n",
    "service_context=ServiceContext.from_defaults(llm=PaLM())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Document & Node Parser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we can load each document manually and add metadata and other fields like relationship after loading the documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "document2 = Document(text=\"./data/script.txt\", metadata={\"filename\": \"script.txt\", \"category\": \"paul_graham_essay\"})\n",
    "documents[0].metadata = {\"category\": \"paul_graham_essays\"}\n",
    "print(documents[0].metadata)\n",
    "print(documents[0].id_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating shared Storage from nodes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can build multiple indices from this shared storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "storage_context = StorageContext.from_defaults()\n",
    "storage_context.docstore.add_documents(nodes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Building Index (saved to disk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Only run once at the beggining\n",
    "vector_index = VectorStoreIndex(nodes, storage_context=storage_context)\n",
    "summary_index = SummaryIndex(nodes, storage_context=storage_context)\n",
    "vector_index.set_index_id(\"vector_index\")\n",
    "summary_index.set_index_id(\"summary_index\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "saving the storage (with 2 indices) on disk - root folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "storage_context.persist(persist_dir=\"./storage\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating the Query Engine \n",
    "Rebuilding the index from our shared storage facility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "storage_context = StorageContext.from_defaults(persist_dir=\"./storage\")\n",
    "vector_index = load_index_from_storage(storage_context, index_id=\"vector_index\")\n",
    "summary_index = load_index_from_storage(storage_context, index_id=\"summary_index\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Configure the Retriever & Build Query Engine\n",
    "An index can have a variety of index-specific retrieval modes. For instance, a summary index supports the default SummaryIndexRetriever that retrieves all nodes, and SummaryIndexEmbeddingRetriever that retrieves the top-k nodes by embedding similarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SummaryIndexRetriever\n",
    "summary_retriever = summary_index.as_retriever(retriever_mode='default')\n",
    "summary_query_engine = RetrieverQueryEngine(summary_retriever)\n",
    "\n",
    "\n",
    "# SummaryIndexEmbeddingRetriever\n",
    "# retriever = summary_index.as_retriever(retriever_mode='embedding')\n",
    "\n",
    "#VectroIndexRetriever\n",
    "vector_retriever = VectorIndexRetriever(index=vector_index, similarity_top_k=3)\n",
    "response_synthesizer = get_response_synthesizer()\n",
    "\n",
    "vector_query_engine = RetrieverQueryEngine(retriever=vector_retriever,\n",
    "                                           response_synthesizer=response_synthesizer,\n",
    "                                           node_postprocessors=[SimilarityPostprocessor(similarity_cutoff=0.7)])\n",
    "\n",
    "# default option\n",
    "# summary_query_engine = summary_index.as_query_engine()\n",
    "# vector_query_engine= vector_index.as_query_engine()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run Queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here is the response: \n",
      " People love Langchain because it makes their life as a developer easier. It provides abstractions that allow for easy switching between vector datastores or embeddings with just a small code change. It also simplifies tasks like chunking files and loading them into a vector store. While there may be some challenges in getting started with Langchain, the documentation has improved and it is appreciated by many developers. Additionally, Langchain offers pluggability, which is useful for supporting different vendor LLMs and avoiding vendor lock-in. Overall, Langchain is seen as a valuable tool for developers in the LLM ecosystem.\n",
      " Here is the source nodes:\n",
      " > Source (Doc id: 8f7fdf70-18bb-4dfe-91e0-d44cad8eaf6b): We're working on building separate tooling to assist with this that we hope to launch soon.  morg...\n",
      "\n",
      "> Source (Doc id: 6d9ead2a-c1e1-4737-b19a-1eb47d13b130): handonam 3 months ago  \n",
      "              \n",
      "\n",
      "the consistency and conventionality of the methods in the...\n",
      "\n",
      "> Source (Doc id: e66ce032-cf4b-4469-ae74-2eb3ef7f6220): It's great you're helping get the word out about LangChain to more people though.  PUSH_AX 3 mont...\n"
     ]
    }
   ],
   "source": [
    "response = vector_query_engine.query(\"why do people love langchain?\")\n",
    "print(f\"Here is the response: \\n {response}\\n Here is the source nodes:\\n {response.get_formatted_sources()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here is the response: \n",
      " The text is a discussion among individuals regarding their concerns and experiences with using a framework called LangChain. Some express frustration with the difficulty of debugging LangChain and the lack of transparency in its functionality. Others mention their reluctance to use LangChain for large complex projects due to its limitations. However, there is also a contrasting viewpoint that praises LangChain for its ability to merge different tactics from the ecosystem into a simpler and more intuitive framework. The text concludes with a playful narrative about a critic who initially dismisses LangChain but eventually realizes their own biases and narrow perspective.\n",
      " Here is the source nodes:\n",
      " > Source (Doc id: 2daff2ab-b77b-4ccb-ac11-2bfe3686aad7): messages.  ntindle 3 months ago  \n",
      "              \n",
      "\n",
      "This is our worry with building Auto-GPT as wel...\n"
     ]
    }
   ],
   "source": [
    "response = summary_query_engine.query(\"summarize the text\")\n",
    "print(f\"Here is the response: \\n {response}\\n Here is the source nodes:\\n {response.get_formatted_sources()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llms",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
